[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Teaching Activity",
    "section": "",
    "text": "Welcome to my teaching materials repository. Here you can find materials from my courses at HSE University."
  },
  {
    "objectID": "index.html#currently-active-courses",
    "href": "index.html#currently-active-courses",
    "title": "Teaching Activity",
    "section": "Currently Active Courses",
    "text": "Currently Active Courses\nThese courses are currently being taught and materials are updated regularly:\n\nProbability Theory and Mathematical Statistics - HSE FCS, Master of Data Science, Spring-Summer 2026."
  },
  {
    "objectID": "index.html#archive",
    "href": "index.html#archive",
    "title": "Teaching Activity",
    "section": "Archive",
    "text": "Archive\nRecent past courses:\n\nLinear Algebra 2025 - HSE FCS, Master of Data Science, Autumn 2025.\nProbability and Mathematical Statistics 2025 - HSE GSB, Business Informatics, Autumn 2025.\n\nFor all past courses and materials, see the Archive page."
  },
  {
    "objectID": "index.html#shared-materials",
    "href": "index.html#shared-materials",
    "title": "Teaching Activity",
    "section": "Shared Materials",
    "text": "Shared Materials\n\nMathematical Statistics Shared Materials\nProbability Theory Shared Materials"
  },
  {
    "objectID": "math_stat_shared/numerical.html",
    "href": "math_stat_shared/numerical.html",
    "title": "üêç Numerical examples",
    "section": "",
    "text": "Python run file"
  },
  {
    "objectID": "math_stat_shared/numerical.html#central-limit-theorem-visualisation",
    "href": "math_stat_shared/numerical.html#central-limit-theorem-visualisation",
    "title": "üêç Numerical examples",
    "section": "",
    "text": "Python run file"
  },
  {
    "objectID": "math_stat_shared/pt_links.html",
    "href": "math_stat_shared/pt_links.html",
    "title": "üîß Useful links",
    "section": "",
    "text": "Table for the standard normal distribution values: üîÆ.\nTables for the Student‚Äôs t-distribution values: ‚û°, and ‚û°.\nCalculator of the t-score: üíª.\n\n\n\n\n\nMIT brief but well-written review on joint distributions\nNice introductory coverage of confidence intervals\nNote that in the subtopic ‚ÄúComparing Two Means‚Äù there is given the most complicated case, when we do not know variances \\(\\sigma_1^2\\) and \\(\\sigma_2^2\\) of two populations, moreover they may not be equal to each other. We didn‚Äôt cover this case, and it‚Äôs better here to follow our materials.\nNice introductory coverage of hypothesis testing, which can help you to look at the HT from slightly another position of the different author. Please, also note here, that there is not full coverage of the subtopic ‚ÄúTests on Differences of Population Means‚Äù. It is better here, as well, to follow our materials.\nYou can find interesting articles on the website statisticsbyjim.com. Among others I can recommend one graphically representing \\(\\alpha\\) and \\(p\\)-values, article about how hypothesis testing works in general, and the article devoted to significance levels."
  },
  {
    "objectID": "math_stat_shared/pt_links.html#tables-and-cdf-calculation-tools",
    "href": "math_stat_shared/pt_links.html#tables-and-cdf-calculation-tools",
    "title": "üîß Useful links",
    "section": "",
    "text": "Table for the standard normal distribution values: üîÆ.\nTables for the Student‚Äôs t-distribution values: ‚û°, and ‚û°.\nCalculator of the t-score: üíª."
  },
  {
    "objectID": "math_stat_shared/pt_links.html#external-recommended-information-sources",
    "href": "math_stat_shared/pt_links.html#external-recommended-information-sources",
    "title": "üîß Useful links",
    "section": "",
    "text": "MIT brief but well-written review on joint distributions\nNice introductory coverage of confidence intervals\nNote that in the subtopic ‚ÄúComparing Two Means‚Äù there is given the most complicated case, when we do not know variances \\(\\sigma_1^2\\) and \\(\\sigma_2^2\\) of two populations, moreover they may not be equal to each other. We didn‚Äôt cover this case, and it‚Äôs better here to follow our materials.\nNice introductory coverage of hypothesis testing, which can help you to look at the HT from slightly another position of the different author. Please, also note here, that there is not full coverage of the subtopic ‚ÄúTests on Differences of Population Means‚Äù. It is better here, as well, to follow our materials.\nYou can find interesting articles on the website statisticsbyjim.com. Among others I can recommend one graphically representing \\(\\alpha\\) and \\(p\\)-values, article about how hypothesis testing works in general, and the article devoted to significance levels."
  },
  {
    "objectID": "archive/hse_prob_stat_21/prob_stat_21.html",
    "href": "archive/hse_prob_stat_21/prob_stat_21.html",
    "title": "Probability Theory and Statistics 2021",
    "section": "",
    "text": "HSE MDI, Autumn 2021.\n\nStatistics course program"
  },
  {
    "objectID": "archive/hse_prob_stat_21/prob_stat_21.html#probability-theory-and-statistics",
    "href": "archive/hse_prob_stat_21/prob_stat_21.html#probability-theory-and-statistics",
    "title": "Probability Theory and Statistics 2021",
    "section": "",
    "text": "HSE MDI, Autumn 2021.\n\nStatistics course program"
  },
  {
    "objectID": "archive/ptms_bi_hse_25/notebooks/crv_animation.html",
    "href": "archive/ptms_bi_hse_25/notebooks/crv_animation.html",
    "title": "",
    "section": "",
    "text": "CodeShow All CodeHide All Code\n\n\n\n\n\n\nCode\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport matplotlib.animation as animation\nfrom matplotlib.patches import ConnectionPatch, Rectangle\nfrom scipy.stats import uniform as un\nfrom functools import partial\n\n\n\n\nCode\ndef pdf(x):\n    if (x &lt; 1):\n        return 0\n    elif (x &gt;= 1) and (x &lt;= 5):\n        return (5 / 4) * (1 / (x ** 2))\n    else:\n        return 0\n\ndef cdf(x):\n    if (x &lt; 1):\n        return 0\n    elif (x &gt;= 1) and (x &lt;= 5):\n        return (5*x - 5) / (4 * x)\n    else:\n        return 1\n\n\nfig, (ax_pdf, ax_cdf) = plt.subplots(\n    nrows=2,\n    sharex=True,\n    figsize=(6, 6),\n)\n\nplt.rcParams['axes.grid'] = True\nplt.rc('legend', fontsize=15)\nfig.suptitle('In class example', fontsize=16)\nx = np.arange(-1, 8, 0.01)\nNx = x.shape[0]\ny_pdf = np.empty((Nx))\ny_cdf = np.empty((Nx))\nfor i in range(Nx):\n    y_pdf[i] = pdf(x[i])\nfor i in range(Nx):\n    y_cdf[i] = cdf(x[i])\nax_cdf.plot(x, y_cdf, label=r'$F_X(x)$')\nax_pdf.plot(x, y_pdf, label=r'$f_X(x)$')\n\nax_pdf.legend()\nax_cdf.legend()\n\n# plt.show()\n# plt.savefig(\"pdf_cdf_example.pdf\")\n\nY1 = y_pdf\nY2 = np.zeros(x.shape[0])\n\nx_a, y_a = x[300], cdf(x[300])\nx_b, y_b = x[500], cdf(x[500])\n\n\n# draw full curve to set view limits in right Axes\n# Plot the 0th frame\nfill_a = ax_pdf.fill_between(x[:300], Y1[:300], color='m', alpha=0.6, label=r'$P(X &lt; 2)$ = F_X(2)')\nfill = ax_pdf.fill_between(x[:500], Y1[:500], color='g', alpha=0.4, label=r'$P(X &lt; 4)$ = F_X(4)')\npoint, = ax_cdf.plot(x[0], 0, \"o\", color='m')\npoint.set_data([x_a], [y_a])\npoint_b, = ax_cdf.plot(x[0], 0, \"o\", color='g')\npoint_b.set_data([x_b], [y_b])\n\n# draw connecting line between both graphs\ncon = ConnectionPatch(\n    (x[0], 0),\n    (x[0], 0),\n    \"data\",\n    \"data\",\n    axesA=ax_cdf,\n    axesB=ax_pdf,\n    color=\"m\",\n    ls=\"dotted\",\n)\ncon.xy1 = x_a, y_a\ncon.xy2 = x_a, 0\nfig.add_artist(con)\n\n# draw connecting line between both graphs\ncon_b = ConnectionPatch(\n    (x[0], 0),\n    (x[0], 0),\n    \"data\",\n    \"data\",\n    axesA=ax_cdf,\n    axesB=ax_pdf,\n    color=\"g\",\n    ls=\"dotted\",\n)\ncon_b.xy1 = x_b, y_b\ncon_b.xy2 = x_b, 0\nfig.add_artist(con_b)\n\nglobal tmp_vert\ntmp_vert = np.zeros((2,2))\ntmp_vert[0, 0], tmp_vert[0,0] = x[0], 0\ntmp_vert[1, 0], tmp_vert[1,1] = x[0], 0\nfill.set_paths([tmp_vert], False)\n\n\ndef animate(i):\n    x = np.arange(-1, 8, 0.01)\n    add_vert = np.array([x[i], 0])\n    path = fill.get_paths()[0]\n    vertices = path.vertices\n    vertices[-1,0], vertices[-1,1] = x[i], pdf(x[i])\n    vertices = np.vstack((vertices, add_vert))\n    fill.set_paths([vertices], False)\n    \n\n    # # Elements 1:Nx+1 encode the upper curve\n    # vertices[1:i + 1, 1] = Y1[:i]\n    x, y = x[i], cdf(x[i])\n    point.set_data([x], [y])\n    con.xy1 = x, y\n    con.xy2 = x, 0\n    return point, fill, con\n\nfr = np.arange(Nx)\nani = animation.FuncAnimation(\n    fig,\n    animate,\n    interval=7,\n    blit=False,  # blitting can't be used with Figure artists\n    frames=fr,\n    repeat_delay=100,\n)\n\n# plt.show()\n\nf = \"in_class.gif\"\nwritergif = animation.PillowWriter(fps=15)\nani.save(f, writer=writergif)\n\n\n\n\nCode\nx[300]\n\n\n\n\nCode\nrv = un(0, 10)\n\nfig, (ax_obj, ax_pdf, ax_cdf) = plt.subplots(\n    nrows=3,\n    sharex=True,\n    figsize=(6, 6),\n)\nstrip = Rectangle((0, 0), 10, 0.2, ec=\"none\")\n# ax_pdf.yaxis.set_visible(False)\nax_pdf.set_ylabel(r'$\\rho(x)$', rotation='horizontal', labelpad=12)\nax_cdf.set_ylabel(r'$m(x)$', rotation='horizontal', labelpad=12)\nfig.suptitle('Physical example', fontsize=16)\nx = np.arange(-3, 13.1, 0.1)\nNx = x.shape[0]\nax_obj.get_yaxis().set_visible(False)\nax_obj.add_artist(strip)\ny_pdf = rv.pdf(x)\ny_cdf = rv.cdf(x)\ncdf, = ax_cdf.plot(x, y_cdf, \"k\")\npdf, = ax_pdf.plot(x, y_pdf)\n\nY1 = y_pdf\nY2 = np.zeros(x.shape[0])\n\n# draw full curve to set view limits in right Axes\n# Plot the 0th frame\n\nfill = ax_pdf.fill_between(x[:2], Y1[:2], color='r', alpha=0.5)\npath = fill.get_paths()[0]\nvertices = path.vertices\nprint(vertices)\npoint, = ax_cdf.plot(x[0], 0, \"o\", color='r')\n\n# draw connecting line between both graphs\ncon = ConnectionPatch(\n    (x[0], 0),\n    (x[0], 0),\n    \"data\",\n    \"data\",\n    axesA=ax_cdf,\n    axesB=ax_pdf,\n    color=\"r\",\n    ls=\"dotted\",\n)\nfig.add_artist(con)\nglobal tmp_vert\ntmp_vert = np.zeros((2,2))\ntmp_vert[0, 0], tmp_vert[0,0] = x[0], 0\ntmp_vert[1, 0], tmp_vert[1,1] = x[0], 0\nfill.set_paths([tmp_vert], False)\n\n\ndef animate(i):\n    # pdf.set_data(x_dom, norm.pdf(x_dom))\n    rv = un(0, 10)\n    x = np.arange(-3, 13.1, 0.1)\n    add_vert = np.array([x[i], 0])\n    path = fill.get_paths()[0]\n    vertices = path.vertices\n    vertices[-1,0], vertices[-1,1] = x[i], rv.pdf(x[i])\n    vertices = np.vstack((vertices, add_vert))\n    fill.set_paths([vertices], False)\n    \n\n    # # Elements 1:Nx+1 encode the upper curve\n    # vertices[1:i + 1, 1] = Y1[:i]\n    x, y = x[i], rv.cdf(x[i])\n    point.set_data([x], [y])\n    con.xy1 = x, y\n    con.xy2 = x, 0\n    return point, fill, con\n\nfr = np.arange(Nx)\nani = animation.FuncAnimation(\n    fig,\n    animate,\n    interval=7,\n    blit=False,  # blitting can't be used with Figure artists\n    frames=fr,\n    repeat_delay=100,\n)\n\n# plt.show()\n\nf = \"pic_distrib.gif\"\nwritergif = animation.PillowWriter(fps=10)\nani.save(f, writer=writergif)\n\n\n\n\nCode\nl = 5.6+1.53333\ndef pdf(x):\n    l = 5.6+1.53333\n    if (x &lt; 0):\n        return 0\n    elif (x&gt;=0) and (x&lt;4.4):\n        return 0.12\n    elif (x&gt;=4.4) and (x&lt;=5.6):\n        return (-0.5 * (x**2) + 5*x - 12.2)\n    elif (x&gt;5.6) and (x&lt;l):\n        return 0.12\n    else:\n        return 0\n\n\ndef cdf(x):\n    l = 5.6+1.53333\n    if (x &lt; 0):\n        return 0\n    elif (x&gt;=0) and (x&lt;=4.4):\n        return 0.12 * x\n    elif (x&gt;4.4) and (x&lt;=5.6):\n        return (cdf(4.4) + -0.166667 * (x**3) + 2.5 * (x**2) - 12.2 * x + 19.4773)\n    elif (x&gt;5.6) and (x&lt;l):\n        return cdf(5.6) + (x-5.6) * 0.12\n    else:\n        return 1\n\n\n\n\nCode\ncdf(5.7)\n\n\n\n\nCode\nfig, (ax_obj, ax_pdf, ax_cdf) = plt.subplots(\n    nrows=3,\n    sharex=True,\n    figsize=(6, 6),\n)\nstrip = Rectangle((0, 0), l, 0.2, ec=\"none\")\n# ax_pdf.yaxis.set_visible(False)\nax_pdf.set_ylabel(r'$\\rho(x)$', rotation='horizontal', labelpad=12)\nax_cdf.set_ylabel(r'$m(x)$', rotation='horizontal', labelpad=12)\nfig.suptitle('Physical example 2', fontsize=16)\nx = np.arange(-3, 10, 0.1)\nNx = x.shape[0]\ny_pdf = np.empty((Nx))\ny_cdf = np.empty((Nx))\nax_obj.get_yaxis().set_visible(False)\nax_obj.add_artist(strip)\nfor i in range(Nx):\n    y_pdf[i] = pdf(x[i])\nfor i in range(Nx):\n    y_cdf[i] = cdf(x[i])\n\ncdf_plot, = ax_cdf.plot(x, y_cdf, \"k\")\npdf_plot, = ax_pdf.plot(x, y_pdf)\n\nY1 = y_pdf\nY2 = np.zeros(x.shape[0])\n\n# draw full curve to set view limits in right Axes\n# Plot the 0th frame\nfill = ax_pdf.fill_between(x[:1], Y1[:1], color='r', alpha=0.5)\npoint, = ax_cdf.plot(x[0], 0, \"o\", color='r')\n\n# draw connecting line between both graphs\ncon = ConnectionPatch(\n    (x[0], 0),\n    (x[0], 0),\n    \"data\",\n    \"data\",\n    axesA=ax_cdf,\n    axesB=ax_pdf,\n    color=\"r\",\n    ls=\"dotted\",\n)\nfig.add_artist(con)\n\nglobal tmp_vert\ntmp_vert = np.zeros((2,2))\ntmp_vert[0, 0], tmp_vert[0,0] = x[0], 0\ntmp_vert[1, 0], tmp_vert[1,1] = x[0], 0\nfill.set_paths([tmp_vert], False)\n\n\ndef animate(i):\n    # pdf.set_data(x_dom, norm.pdf(x_dom))\n    x = np.arange(-3, 10, 0.1)\n    add_vert = np.array([x[i], 0])\n    path = fill.get_paths()[0]\n    vertices = path.vertices\n    vertices[-1,0], vertices[-1,1] = x[i], pdf(x[i])\n    vertices = np.vstack((vertices, add_vert))\n    fill.set_paths([vertices], False)\n    \n\n    # # Elements 1:Nx+1 encode the upper curve\n    # vertices[1:i + 1, 1] = Y1[:i]\n    x, y = x[i], cdf(x[i])\n    point.set_data([x], [y])\n    con.xy1 = x, y\n    con.xy2 = x, 0\n    return point, fill, con\n\nfr = np.arange(Nx)\nani = animation.FuncAnimation(\n    fig,\n    animate,\n    interval=7,\n    blit=False,  # blitting can't be used with Figure artists\n    frames=fr,\n    repeat_delay=100,\n)\n\n# plt.show()\n\nf = \"strip_with_incl.gif\"\nwritergif = animation.PillowWriter(fps=10)\nani.save(f, writer=writergif)\n\n\n\n\nCode\ndef pdf(x):\n    if (x &lt; 0):\n        return 0\n    elif (x&gt;=0) and (x&lt;=2):\n        return (x**3) / 4\n    else:\n        return 0\n\n\ndef cdf(x):\n    if (x &lt; 0):\n        return 0\n    elif (x&gt;=0) and (x&lt;=2):\n        return (x ** 4) / 16\n    else:\n        return 1\n\nfig, (ax_pdf, ax_cdf) = plt.subplots(\n    nrows=2,\n    sharex=True,\n    figsize=(6, 6),\n)\n\n# ax_pdf.yaxis.set_visible(False)\nax_pdf.set_ylabel(r'$f_X(x)$', rotation='horizontal', labelpad=12)\nax_cdf.set_ylabel(r'$F_X(x)$', rotation='horizontal', labelpad=12)\nfig.suptitle(r'$x^3 / 4$ density function', fontsize=16)\nx = np.arange(-2, 6, 0.05)\nNx = x.shape[0]\ny_pdf = np.empty((Nx))\ny_cdf = np.empty((Nx))\n\nfor i in range(Nx):\n    y_pdf[i] = pdf(x[i])\nfor i in range(Nx):\n    y_cdf[i] = cdf(x[i])\n\ncdf_plot, = ax_cdf.plot(x, y_cdf, \"k\")\npdf_plot, = ax_pdf.plot(x, y_pdf)\n\nY1 = y_pdf\nY2 = np.zeros(x.shape[0])\n\n# draw full curve to set view limits in right Axes\n# Plot the 0th frame\nfill = ax_pdf.fill_between(x[:1], Y1[:1], color='r', alpha=0.5)\npoint, = ax_cdf.plot(x[0], 0, \"o\", color='r')\n\n# draw connecting line between both graphs\ncon = ConnectionPatch(\n    (x[0], 0),\n    (x[0], 0),\n    \"data\",\n    \"data\",\n    axesA=ax_cdf,\n    axesB=ax_pdf,\n    color=\"r\",\n    ls=\"dotted\",\n)\nfig.add_artist(con)\n\nglobal tmp_vert\ntmp_vert = np.zeros((2,2))\ntmp_vert[0, 0], tmp_vert[0,0] = x[0], 0\ntmp_vert[1, 0], tmp_vert[1,1] = x[0], 0\nfill.set_paths([tmp_vert], False)\n\n\ndef animate(i):\n    # pdf.set_data(x_dom, norm.pdf(x_dom))\n    x = np.arange(-2, 6, 0.05)\n    add_vert = np.array([x[i], 0])\n    path = fill.get_paths()[0]\n    vertices = path.vertices\n    vertices[-1,0], vertices[-1,1] = x[i], pdf(x[i])\n    vertices = np.vstack((vertices, add_vert))\n    fill.set_paths([vertices], False)\n    \n\n    # # Elements 1:Nx+1 encode the upper curve\n    # vertices[1:i + 1, 1] = Y1[:i]\n    x, y = x[i], cdf(x[i])\n    point.set_data([x], [y])\n    con.xy1 = x, y\n    con.xy2 = x, 0\n    return point, fill, con\n\nfr = np.arange(Nx)\nani = animation.FuncAnimation(\n    fig,\n    animate,\n    interval=7,\n    blit=False,  # blitting can't be used with Figure artists\n    frames=fr,\n    repeat_delay=100,\n)\n\n# plt.show()\n\nf = \"3a.gif\"\nwritergif = animation.PillowWriter(fps=10)\nani.save(f, writer=writergif)\n\n\n\n\nCode\ndef pdf(x):\n    if (x &lt; 0):\n        return 0\n    elif (x&gt;=0) and (x&lt;=1):\n        return 2 * x\n    else:\n        return 0\n\n\ndef cdf(x):\n    if (x &lt; 0):\n        return 0\n    elif (x&gt;=0) and (x&lt;=1):\n        return (x ** 2)\n    else:\n        return 1\n\nfig, (ax_pdf, ax_cdf) = plt.subplots(\n    nrows=2,\n    sharex=True,\n    figsize=(6, 6),\n)\n\n# ax_pdf.yaxis.set_visible(False)\nax_pdf.set_ylabel(r'$f_X(x)$', rotation='horizontal', labelpad=12)\nax_cdf.set_ylabel(r'$F_X(x)$', rotation='horizontal', labelpad=12)\nfig.suptitle(r'$2x$ density function', fontsize=16)\nx = np.arange(-2, 6, 0.05)\nNx = x.shape[0]\ny_pdf = np.empty((Nx))\ny_cdf = np.empty((Nx))\n\nfor i in range(Nx):\n    y_pdf[i] = pdf(x[i])\nfor i in range(Nx):\n    y_cdf[i] = cdf(x[i])\n\ncdf_plot, = ax_cdf.plot(x, y_cdf, \"k\")\npdf_plot, = ax_pdf.plot(x, y_pdf)\n\nY1 = y_pdf\nY2 = np.zeros(x.shape[0])\n\n# draw full curve to set view limits in right Axes\n# Plot the 0th frame\nfill = ax_pdf.fill_between(x[:1], Y1[:1], color='r', alpha=0.5)\npoint, = ax_cdf.plot(x[0], 0, \"o\", color='r')\n\n# draw connecting line between both graphs\ncon = ConnectionPatch(\n    (x[0], 0),\n    (x[0], 0),\n    \"data\",\n    \"data\",\n    axesA=ax_cdf,\n    axesB=ax_pdf,\n    color=\"r\",\n    ls=\"dotted\",\n)\nfig.add_artist(con)\n\nglobal tmp_vert\ntmp_vert = np.zeros((2,2))\ntmp_vert[0, 0], tmp_vert[0,0] = x[0], 0\ntmp_vert[1, 0], tmp_vert[1,1] = x[0], 0\nfill.set_paths([tmp_vert], False)\n\n\ndef animate(i):\n    # pdf.set_data(x_dom, norm.pdf(x_dom))\n    x = np.arange(-2, 6, 0.05)\n    add_vert = np.array([x[i], 0])\n    path = fill.get_paths()[0]\n    vertices = path.vertices\n    vertices[-1,0], vertices[-1,1] = x[i], pdf(x[i])\n    vertices = np.vstack((vertices, add_vert))\n    fill.set_paths([vertices], False)\n    \n\n    # # Elements 1:Nx+1 encode the upper curve\n    # vertices[1:i + 1, 1] = Y1[:i]\n    x, y = x[i], cdf(x[i])\n    point.set_data([x], [y])\n    con.xy1 = x, y\n    con.xy2 = x, 0\n    return point, fill, con\n\nfr = np.arange(Nx)\nani = animation.FuncAnimation(\n    fig,\n    animate,\n    interval=7,\n    blit=False,  # blitting can't be used with Figure artists\n    frames=fr,\n    repeat_delay=100,\n)\n\n# plt.show()\n\nf = \"1.gif\"\nwritergif = animation.PillowWriter(fps=10)\nani.save(f, writer=writergif)\n\n\n\n\nCode\ndef pdf(x):\n    if (x &lt; 10):\n        return 0\n    else:\n        return (10 / x ** 2)\n\n\ndef cdf(x):\n    if (x &lt; 10):\n        return 0\n    else:\n        return (1 - 10 / x)\n\nfig, (ax_pdf, ax_cdf) = plt.subplots(\n    nrows=2,\n    sharex=True,\n    figsize=(6, 6),\n)\n\n# ax_pdf.yaxis.set_visible(False)\nax_pdf.set_ylabel(r'$f_X(x)$', rotation='horizontal', labelpad=12)\nax_cdf.set_ylabel(r'$F_X(x)$', rotation='horizontal', labelpad=12)\nfig.suptitle(r'$10 / x^2$ density function', fontsize=16)\nx = np.arange(8, 40, 0.25)\nNx = x.shape[0]\ny_pdf = np.empty((Nx))\ny_cdf = np.empty((Nx))\n\nfor i in range(Nx):\n    y_pdf[i] = pdf(x[i])\nfor i in range(Nx):\n    y_cdf[i] = cdf(x[i])\n\ncdf_plot, = ax_cdf.plot(x, y_cdf, \"k\")\npdf_plot, = ax_pdf.plot(x, y_pdf)\n\nY1 = y_pdf\nY2 = np.zeros(x.shape[0])\n\n# draw full curve to set view limits in right Axes\n# Plot the 0th frame\nfill = ax_pdf.fill_between(x[:1], Y1[:1], color='r', alpha=0.5)\npoint, = ax_cdf.plot(x[0], 0, \"o\", color='r')\n\n# draw connecting line between both graphs\ncon = ConnectionPatch(\n    (x[0], 0),\n    (x[0], 0),\n    \"data\",\n    \"data\",\n    axesA=ax_cdf,\n    axesB=ax_pdf,\n    color=\"r\",\n    ls=\"dotted\",\n)\nfig.add_artist(con)\n\nglobal tmp_vert\ntmp_vert = np.zeros((2,2))\ntmp_vert[0, 0], tmp_vert[0,0] = x[0], 0\ntmp_vert[1, 0], tmp_vert[1,1] = x[0], 0\nfill.set_paths([tmp_vert], False)\n\n\ndef animate(i):\n    # pdf.set_data(x_dom, norm.pdf(x_dom))\n    x = np.arange(8, 40, 0.25)\n    add_vert = np.array([x[i], 0])\n    path = fill.get_paths()[0]\n    vertices = path.vertices\n    vertices[-1,0], vertices[-1,1] = x[i], pdf(x[i])\n    vertices = np.vstack((vertices, add_vert))\n    fill.set_paths([vertices], False)\n    \n\n    # # Elements 1:Nx+1 encode the upper curve\n    # vertices[1:i + 1, 1] = Y1[:i]\n    x, y = x[i], cdf(x[i])\n    point.set_data([x], [y])\n    con.xy1 = x, y\n    con.xy2 = x, 0\n    return point, fill, con\n\nfr = np.arange(Nx)\nani = animation.FuncAnimation(\n    fig,\n    animate,\n    interval=7,\n    blit=False,  # blitting can't be used with Figure artists\n    frames=fr,\n    repeat_delay=100,\n)\n\n# plt.show()\n\nf = \"2.gif\"\nwritergif = animation.PillowWriter(fps=10)\nani.save(f, writer=writergif)"
  },
  {
    "objectID": "archive/ptms_bi_hse_25/notebooks/clt.html",
    "href": "archive/ptms_bi_hse_25/notebooks/clt.html",
    "title": "Frequentist probability",
    "section": "",
    "text": "Code\nimport math\nimport seaborn as sns\nimport numpy as np\nfrom matplotlib import pyplot as plt\nimport scipy.stats as stats\nfrom scipy.stats import randint, norm"
  },
  {
    "objectID": "archive/ptms_bi_hse_25/notebooks/clt.html#m√©thode-de-monte-carlo",
    "href": "archive/ptms_bi_hse_25/notebooks/clt.html#m√©thode-de-monte-carlo",
    "title": "Frequentist probability",
    "section": "M√©thode de Monte-Carlo",
    "text": "M√©thode de Monte-Carlo\nFor some complex scenarios it may be impossible to know distribution a priori. To get some insights we can use frequency approach: perform many experiments and define probability of event as the limit of its relative frequency to the total number of experiments.\nFor example let us consider frequential experiment with the 6-sided die, as if we don‚Äôt know probabilities of obtaining each side.\nIf we deal with continuous random variable we also can use histogram to at least approximate form of the density function.\n\n\nCode\nn_draws = 36000\nrv = randint(low=1,high=7)\nresult = np.empty(n_draws)\n\nfor i in range(n_draws):\n    result[i] = rv.rvs(1)[0]\n\nplt.figure(figsize=(10, 5))\nsns.histplot(result)\n\n\nIn the same way we can try to analyze approximate form of the density function for continuous random variables.\nSince they have infinitely many values, we divide all available space in bins, and just compute how many times our random variable of interest fell in each bin.\n\n\nCode\nb = stats.norm(0, 1)\n\n# Find out global properties of population:\nmu = b.mean()\nvar = b.var()\n\n# Generate Sample Means\nn_draws = 100000\nresult = np.empty(n_draws)\nn_bins = 60\n\nfor i in range(n_draws):\n    result[i] = b.rvs(1)[0]   \n\nplt.figure(figsize=(10, 5))\nsns.histplot(result, bins=n_bins).grid()\ncounts, _, _ = plt.hist(result, bins=n_bins, alpha=0.0)  # just in order to find out the scaling coefficient for PDF\nplt.title('Histogram for standard normal variable')\n\n# scaling of normal PDF is needed, because histogram has large values on y-axis, and we need to fit them\nx_space = np.linspace(-5, 5)\nplt.plot(x_space, np.max(counts) * stats.norm.pdf(x_space, 0, 1) * np.sqrt(2 * np.pi), label='Normal density')\nplt.legend()\nplt.show()"
  },
  {
    "objectID": "archive/ptms_bi_hse_25/notebooks/clt.html#transition-to-the-central-limit-theorem-2-dices",
    "href": "archive/ptms_bi_hse_25/notebooks/clt.html#transition-to-the-central-limit-theorem-2-dices",
    "title": "Frequentist probability",
    "section": "Transition to the Central Limit Theorem: 2 dices",
    "text": "Transition to the Central Limit Theorem: 2 dices\nLet us return to the first example with 6-sided dice. But for now suppose we have 2 independent dices and we now observe two random variables at once: discrete bivariate vector \\((X,Y)\\).\nNew random variable \\(T = f(X,Y) = X + Y\\) is clearly a function from \\(X\\) and \\(Y\\) and also is a random variables itself. It is possible to obtain its probability mass function (PMF) theoretically, but let us apply our simulation and see how the histogram looks like.\n\n\nCode\nn_draws = 36000\nrv = randint(low=1,high=7)\nresult = np.empty(n_draws)\n\n\nfor i in range(n_draws):\n    x = rv.rvs(1)[0]\n    y = rv.rvs(1)[0]\n    result[i] = x + y\n\nplt.figure(figsize=(10, 5))\nsns.histplot(result)"
  },
  {
    "objectID": "archive/ptms_bi_hse_25/notebooks/clt.html#transition-to-the-central-limit-theorem-3-dices",
    "href": "archive/ptms_bi_hse_25/notebooks/clt.html#transition-to-the-central-limit-theorem-3-dices",
    "title": "Frequentist probability",
    "section": "Transition to the Central Limit Theorem: 3 dices",
    "text": "Transition to the Central Limit Theorem: 3 dices\nIn this example we go even further and here we have 3 independent dices, which means we observe three random variables at once: discrete vector \\((X_1, X_2, X_3)\\).\nAgain we are going to look at their sum - random variable \\(T = X_1 + X_2 + X_3\\).\n\n\nCode\nn_draws = 36000\nrv = randint(low=1,high=7)\nresult = np.empty(n_draws)\n\n\nfor i in range(n_draws):\n    x_1 = rv.rvs(1)[0]\n    x_2 = rv.rvs(1)[0]\n    x_3 = rv.rvs(1)[0]\n    result[i] = x_1 + x_2 + x_3\n\nplt.figure(figsize=(10, 5))\nsns.histplot(result)"
  },
  {
    "objectID": "archive/ptms_bi_hse_25/notebooks/clt.html#transition-to-the-central-limit-theorem-5-dices",
    "href": "archive/ptms_bi_hse_25/notebooks/clt.html#transition-to-the-central-limit-theorem-5-dices",
    "title": "Frequentist probability",
    "section": "Transition to the Central Limit Theorem: 5 dices",
    "text": "Transition to the Central Limit Theorem: 5 dices\nIn this example we go even further and here we have 5 independent dices, which means we observe five random variables at once: discrete vector \\((X_1, X_2, X_3, X_4, X_5)\\).\nAgain we are going to look at their sum - random variable \\(T = X_1 + X_2 + X_3 + X_4 + X_5\\).\n\n\nCode\nn_draws = 100000\nrv = randint(low=1,high=7)\nresult = np.empty(n_draws)\n\n\nfor i in range(n_draws):\n    x_1 = rv.rvs(1)[0]\n    x_2 = rv.rvs(1)[0]\n    x_3 = rv.rvs(1)[0]\n    x_4 = rv.rvs(1)[0]\n    x_5 = rv.rvs(1)[0]\n    result[i] = x_1 + x_2 + x_3 + x_4 + x_5\n\nplt.figure(figsize=(10, 5))\nsns.histplot(result)"
  },
  {
    "objectID": "archive/ptms_bi_hse_25/notebooks/clt.html#philosophical-form",
    "href": "archive/ptms_bi_hse_25/notebooks/clt.html#philosophical-form",
    "title": "Frequentist probability",
    "section": "‚ÄòPhilosophical‚Äô form",
    "text": "‚ÄòPhilosophical‚Äô form\n\nLet \\(X_1, \\ldots, X_n\\) be a sequence of independent random variables taken from the same distribution, i.e.¬†all \\(X_i\\)‚Äôs have the same expectation \\(\\mu\\) and finite variance \\(\\sigma^2\\). Let us construct new random variable:\n\n\\[\nS_n = \\sum \\limits_{i=1}^{n} X_i.\n\\]\n\nAccording to the properties of expectation and variance, properties of the new random variable are:\n\n\\[\\begin{gather}\n    E[S_n] = E \\Bigl[ \\sum \\limits_{i=1}^{n} X_i \\Bigr] = \\sum \\limits_{i=1}^{n} E[X_i] = n \\mu \\\\\n    Var[S_n] = Var \\Bigl[ \\sum \\limits_{i=1}^{n} X_i \\Bigr] = \\sum \\limits_{i=1}^{n} Var[X_i] = n \\sigma^2\n\\end{gather}\\]\n\nCentral Limit Theorem: distribution of such random variable \\(S_n\\) tends to normal as \\(n \\rightarrow \\infty\\): \\(S_n  \\rightarrow  Y \\sim \\mathcal{N}(n \\mu, n \\sigma^2)\\). More specifically formulated:\n\n\\[\n  \\forall x \\in \\mathbb{R} \\quad  P(S_n &lt; x) \\underset{n \\rightarrow \\infty}{\\longrightarrow} P(Y &lt; x), \\text{ where } Y \\sim \\mathcal{N}(n \\mu, n \\sigma^2).\n\\]\n\nIn simple words, we can just say that \\(S_n \\sim \\mathcal{N}(n \\mu, n \\sigma^2)\\), when \\(n\\) is sufficiently large.\n\n\nOnce again, sum of many ANY random variables gives us.. normal?\n\n\nLooks like cheap scam!\nTo prove that let‚Äôs take random variable that is faaar from being normal.\nHow on Earth the density function of sum of them can at least remotely resemble normal distribution?\n\n\nCode\nb = stats.beta(0.05, 0.09)\n\n# Find out global properties of population:\nmu = b.mean()\nvar = b.var()\n\n# Generate Sample Means\nn_draws = 10000\nrandom_var = np.empty(n_draws)\nn_bins = 20\n\nfor i in range(n_draws):\n    random_var[i] = np.random.beta(0.05, 0.05)   \n\nplt.figure(figsize=(10, 5))\nsns.histplot(random_var, bins=n_bins).grid()\ncounts, _, _ = plt.hist(random_var, bins=n_bins, alpha=0.0)  # just in order to find out the scaling coefficient for PDF\nplt.title('Histogram for beta random variable')\n\n# scaling of normal PDF is needed, because histogram has large values on y-axis, and we need to fit them\nx_space = np.linspace(-3, 3)\nplt.plot(x_space, np.max(counts) * stats.norm.pdf(x_space, 0, 1) * np.sqrt(2 * np.pi), label='Normal density')\nplt.legend()\nplt.show()\n\n\n\n\nCode\nplt.figure(figsize=(10, 5))\n\nplt.title('True density of beta random variable')\n\n# scaling of normal PDF is needed, because histogram has large values on y-axis, and we need to fit them\nx_space = np.linspace(-1, 3, 10001)\nplt.plot(x_space, b.pdf(x_space), label='beta density')\nplt.grid()\nplt.legend()\nplt.show()\n\n\n\n\nCode\nb = stats.beta(0.05, 0.09)\n\n# Find out global properties of population:\nmu = b.mean()\nsigma = np.sqrt(b.var())\nvar = b.var()\n\nprint(\"Parameters of a single random variable: Mean = {}, Var = {}\".format(mu, var))\n\n# Generate Sample Means\nn_draws = 10000\nx_totals = np.empty(n_draws)\nn_bins = 50\nfor sample_size in range(1, 50):\n    for i in range(n_draws):\n        sample = b.rvs(size=sample_size)\n        x_totals[i] = np.sum(sample)    \n    \n    plt.figure(figsize=(12, 3))\n    sns.histplot(x_totals, bins=n_bins).grid()\n    counts, _, _ = plt.hist(x_totals, bins=n_bins, alpha=0.0)  # just in order to find out the scaling coefficient for PDF\n    if sample_size == 1:\n        plt.title(r'Approximate density shape of variable $(X_1)$')\n    else:\n        plt.title(r'Approximate density shape of variable $(X_1 + \\ldots + X_{%d})$' % (sample_size))\n    \n    # scaling of normal PDF is needed, because histogram has large values on y-axis, and we need to fit them\n    x_space = np.linspace(sample_size * mu - 3 * var * sample_size, sample_size * mu + 3 * var * sample_size, 1000)\n    current_std = np.sqrt(var * sample_size)\n    plt.plot(x_space, np.max(counts) * stats.norm.pdf(x_space, sample_size * mu, current_std) * np.sqrt(2 * np.pi) * current_std, label=r'$\\mathcal{N} \\; \\left(%s \\cdot %s, \\; %s \\sigma^2\\right)$' % (sample_size, mu, sample_size))\n    plt.legend()\n    plt.show()\n    # plt.savefig('plot_{}.png'.format(sample_size))\n    plt.close()"
  },
  {
    "objectID": "archive/ptms_bi_hse_25/notebooks/clt.html#more-widespread-formulation",
    "href": "archive/ptms_bi_hse_25/notebooks/clt.html#more-widespread-formulation",
    "title": "Frequentist probability",
    "section": "More widespread formulation",
    "text": "More widespread formulation\nMore often in textbooks we can meet another formulation of CLT.\nFirstly, recall that if \\(X\\) is any normal random variable, then the following function makes it standard normal:\n\\[\n    Z = \\frac{X - E[X]}{\\sqrt{Var[X]}}, \\quad Z \\sim \\mathcal{N}(0,1)\n\\]\n\nLet \\(X_1, \\ldots, X_n\\) be a sequence of independent random variables taken from the same distribution, i.e.¬†all \\(X_i\\)‚Äôs have the same expectation \\(\\mu\\) and finite variance \\(\\sigma^2\\).\nAs before we construct new random variable: \\[\n  S_n = \\sum \\limits_{i=1}^{n} X_i, \\quad E[S_n] = n \\mu, \\quad Var[S_n] = n \\sigma^2.\n\\]\nLet us apply tranformation to \\(S_n\\): \\[\n  Z_n = \\frac{S_n - E[S_n]}{\\sqrt{Var[S_n]}} = \\frac{S_n - n \\mu}{\\sigma \\sqrt{n}}\n\\]\nProperties of this new random variable: \\[\\begin{gather}\n  E[Z_n] = E \\Bigl[\\frac{S_n}{\\sigma \\sqrt{n}} - \\frac{n \\mu}{\\sigma \\sqrt{n}} \\Bigr] = E \\Bigl[\\frac{S_n}{\\sigma \\sqrt{n}} \\Bigl] - E \\Bigl[\\frac{n \\mu}{\\sigma \\sqrt{n}} \\Bigr] = \\frac{n \\mu}{\\sigma \\sqrt{n}} - \\frac{n \\mu}{\\sigma \\sqrt{n}} = 0 \\\\\n  Var[Z_n] = Var \\Bigl[\\frac{S_n}{\\sigma \\sqrt{n}} - \\frac{n \\mu}{\\sigma \\sqrt{n}} \\Bigr] = Var \\Bigl[\\frac{S_n}{\\sigma \\sqrt{n}} \\Bigl] + Var \\Bigl[\\frac{n \\mu}{\\sigma \\sqrt{n}} \\Bigr] = \\frac{Var[S_n]}{n \\sigma^2} + 0 = 1.\n\\end{gather}\\]\nCentral Limit Theorem: distribution of such random variable \\(Z_n\\) tends to standard normal as \\(n \\rightarrow \\infty\\): \\(Z_n  \\rightarrow  Z \\sim \\mathcal{N}(0, 1)\\).\nIn simple words, if \\(n\\) is large enough, then we can say that \\(S_n \\sim \\mathcal{N}(n \\mu, n \\sigma^2)\\), and simultaneously random variable \\(Z_n = \\frac{S_n - n\\mu}{\\sigma \\sqrt{n}}\\) has standard normal distribution i.e. \\(Z_n \\sim \\mathcal{N}(0, 1)\\).\n\n\n\nCode\nb = stats.beta(0.05, 0.05)\n\n# Find out global properties of population:\nmu = b.mean()\nsigma = np.sqrt(b.var())\nvar = b.var()\n\n# Generate Sample Means\nn_draws = 10000\nx_totals = np.empty(n_draws)\nn_bins = 50\n\nfor sample_size in range(1, 35):\n    for i in range(n_draws):\n        sample = b.rvs(size=sample_size)\n        x_totals[i] = np.sum(sample)    \n\n    # transformation of all realizations to standard normal\n    z = (x_totals - sample_size * mu) / np.sqrt(var * sample_size)\n    plt.figure(figsize=(10, 5))\n    sns.histplot(z, bins=n_bins).grid()\n    counts, _, _ = plt.hist(z, bins=n_bins, alpha=0.0)  # just in order to find out the scaling coefficient for PDF\n    plt.title(r'Approximate density shape of variable $Z_n$ (sample size = {})'.format(sample_size))\n\n    # scaling of standard normal PDF, because histogram has large values on y-axis, and we need to fit them\n    x_space = np.linspace(-5, 5)\n    plt.plot(x_space, np.max(counts) * stats.norm.pdf(x_space, 0, 1) * np.sqrt(2 * np.pi), label='Standard Normal')\n    plt.legend()\n    plt.show()"
  },
  {
    "objectID": "archive/hse_prob_stat_22/stat_22.html",
    "href": "archive/hse_prob_stat_22/stat_22.html",
    "title": "Mathematical Statistics 2022",
    "section": "",
    "text": "## Mathematical Statistics. HSE MDI, Sep ‚Äî Oct 2022."
  },
  {
    "objectID": "archive/hse_prob_stat_22/stat_22.html#course-materials",
    "href": "archive/hse_prob_stat_22/stat_22.html#course-materials",
    "title": "Mathematical Statistics 2022",
    "section": "Course materials",
    "text": "Course materials\n\nStatistics course program\nHome Assignments\n\n * External materials"
  },
  {
    "objectID": "archive/hse_prob_stat_22/stat_program.html",
    "href": "archive/hse_prob_stat_22/stat_program.html",
    "title": "Mathematical Statistics course program üìäüé≤",
    "section": "",
    "text": "Probability revision: distributions. Notes.\n\n\n\nProbability revision: multivariate random variables (MRV). Notes.\n\n\n\nMRV Part II. Continuous MRV. Introduction to Statistics.\nMaterials. Notes.\n\n\n\nSampling distribution. Point estimation. Unbiased estimators. Mean Squared Error.\nMaterials. Notes.\n\n\n\nCentral Limit Theorem. Confidence Intervals. Estimation of population mean, given population variance.\nMaterials. Notes.\n\n\n\nConfidence Intervals: estimation of population proportion, difference between population means, difference between population proportions.\nMaterials. Notes.\n\n\n\nConfidence Intervals: estimation of population mean without population variance. Student‚Äôs t-distribution.\n\nSeminar materials.\nNotes.\nNotes from online seminar.\nTheory we need for understanding chi-squared and t-distribution.\nNotebook with visualisation of density functions\n\n\n\n\nConfidence Intervals cookbook ü•òüç≥üç∑\n\n\n\n\n\nüöß Construction in progress. üöß\n\n\n\nHypotheses Testing: test of population mean with known and unknown variance, test for population proportion.\nMaterials. List of problems.\n\n\n\nHypothese Testing cookbook üçûüßàüç¥"
  },
  {
    "objectID": "archive/hse_prob_stat_22/stat_program.html#seminar-1.",
    "href": "archive/hse_prob_stat_22/stat_program.html#seminar-1.",
    "title": "Mathematical Statistics course program üìäüé≤",
    "section": "",
    "text": "Probability revision: distributions. Notes."
  },
  {
    "objectID": "archive/hse_prob_stat_22/stat_program.html#seminar-2.",
    "href": "archive/hse_prob_stat_22/stat_program.html#seminar-2.",
    "title": "Mathematical Statistics course program üìäüé≤",
    "section": "",
    "text": "Probability revision: multivariate random variables (MRV). Notes."
  },
  {
    "objectID": "archive/hse_prob_stat_22/stat_program.html#seminar-3.",
    "href": "archive/hse_prob_stat_22/stat_program.html#seminar-3.",
    "title": "Mathematical Statistics course program üìäüé≤",
    "section": "",
    "text": "MRV Part II. Continuous MRV. Introduction to Statistics.\nMaterials. Notes."
  },
  {
    "objectID": "archive/hse_prob_stat_22/stat_program.html#seminar-4.",
    "href": "archive/hse_prob_stat_22/stat_program.html#seminar-4.",
    "title": "Mathematical Statistics course program üìäüé≤",
    "section": "",
    "text": "Sampling distribution. Point estimation. Unbiased estimators. Mean Squared Error.\nMaterials. Notes."
  },
  {
    "objectID": "archive/hse_prob_stat_22/stat_program.html#seminar-5.",
    "href": "archive/hse_prob_stat_22/stat_program.html#seminar-5.",
    "title": "Mathematical Statistics course program üìäüé≤",
    "section": "",
    "text": "Central Limit Theorem. Confidence Intervals. Estimation of population mean, given population variance.\nMaterials. Notes."
  },
  {
    "objectID": "archive/hse_prob_stat_22/stat_program.html#seminar-6.",
    "href": "archive/hse_prob_stat_22/stat_program.html#seminar-6.",
    "title": "Mathematical Statistics course program üìäüé≤",
    "section": "",
    "text": "Confidence Intervals: estimation of population proportion, difference between population means, difference between population proportions.\nMaterials. Notes."
  },
  {
    "objectID": "archive/hse_prob_stat_22/stat_program.html#seminar-7.",
    "href": "archive/hse_prob_stat_22/stat_program.html#seminar-7.",
    "title": "Mathematical Statistics course program üìäüé≤",
    "section": "",
    "text": "Confidence Intervals: estimation of population mean without population variance. Student‚Äôs t-distribution.\n\nSeminar materials.\nNotes.\nNotes from online seminar.\nTheory we need for understanding chi-squared and t-distribution.\nNotebook with visualisation of density functions\n\n\n\n\nConfidence Intervals cookbook ü•òüç≥üç∑"
  },
  {
    "objectID": "archive/hse_prob_stat_22/stat_program.html#seminar-8.",
    "href": "archive/hse_prob_stat_22/stat_program.html#seminar-8.",
    "title": "Mathematical Statistics course program üìäüé≤",
    "section": "",
    "text": "üöß Construction in progress. üöß"
  },
  {
    "objectID": "archive/hse_prob_stat_22/stat_program.html#seminar-9.",
    "href": "archive/hse_prob_stat_22/stat_program.html#seminar-9.",
    "title": "Mathematical Statistics course program üìäüé≤",
    "section": "",
    "text": "Hypotheses Testing: test of population mean with known and unknown variance, test for population proportion.\nMaterials. List of problems.\n\n\n\nHypothese Testing cookbook üçûüßàüç¥"
  },
  {
    "objectID": "archive/hse_prob_stat_22/pt_program.html",
    "href": "archive/hse_prob_stat_22/pt_program.html",
    "title": "Probability Theory course program üé∞",
    "section": "",
    "text": "Introduction. Probability space.\n\n\n\nConcept of probability. Classical approach. Materials.\n\n\n\nConditional probability.\n\n\n\nTotal Probability. Bayes rule. Materials.\n\n\n\nCounting principles. Materials.\n\n\n\nDiscrete Random Variables.\n\n\n\nContinuous Random Variables.\n\n\n\nDistributions. Binomial, Uniform, Normal.\n\n\n\nMultivariate discrete random variables.\nNotes."
  },
  {
    "objectID": "archive/hse_prob_stat_22/pt_program.html#seminar-1.",
    "href": "archive/hse_prob_stat_22/pt_program.html#seminar-1.",
    "title": "Probability Theory course program üé∞",
    "section": "",
    "text": "Introduction. Probability space."
  },
  {
    "objectID": "archive/hse_prob_stat_22/pt_program.html#seminar-2.",
    "href": "archive/hse_prob_stat_22/pt_program.html#seminar-2.",
    "title": "Probability Theory course program üé∞",
    "section": "",
    "text": "Concept of probability. Classical approach. Materials."
  },
  {
    "objectID": "archive/hse_prob_stat_22/pt_program.html#seminar-3.",
    "href": "archive/hse_prob_stat_22/pt_program.html#seminar-3.",
    "title": "Probability Theory course program üé∞",
    "section": "",
    "text": "Conditional probability."
  },
  {
    "objectID": "archive/hse_prob_stat_22/pt_program.html#seminar-4.",
    "href": "archive/hse_prob_stat_22/pt_program.html#seminar-4.",
    "title": "Probability Theory course program üé∞",
    "section": "",
    "text": "Total Probability. Bayes rule. Materials."
  },
  {
    "objectID": "archive/hse_prob_stat_22/pt_program.html#seminar-5.",
    "href": "archive/hse_prob_stat_22/pt_program.html#seminar-5.",
    "title": "Probability Theory course program üé∞",
    "section": "",
    "text": "Counting principles. Materials."
  },
  {
    "objectID": "archive/hse_prob_stat_22/pt_program.html#seminar-6.",
    "href": "archive/hse_prob_stat_22/pt_program.html#seminar-6.",
    "title": "Probability Theory course program üé∞",
    "section": "",
    "text": "Discrete Random Variables."
  },
  {
    "objectID": "archive/hse_prob_stat_22/pt_program.html#seminar-7.",
    "href": "archive/hse_prob_stat_22/pt_program.html#seminar-7.",
    "title": "Probability Theory course program üé∞",
    "section": "",
    "text": "Continuous Random Variables."
  },
  {
    "objectID": "archive/hse_prob_stat_22/pt_program.html#seminar-8.",
    "href": "archive/hse_prob_stat_22/pt_program.html#seminar-8.",
    "title": "Probability Theory course program üé∞",
    "section": "",
    "text": "Distributions. Binomial, Uniform, Normal."
  },
  {
    "objectID": "archive/hse_prob_stat_22/pt_program.html#seminar-9-10.",
    "href": "archive/hse_prob_stat_22/pt_program.html#seminar-9-10.",
    "title": "Probability Theory course program üé∞",
    "section": "",
    "text": "Multivariate discrete random variables.\nNotes."
  },
  {
    "objectID": "archive/linalg_mds_25/index.html",
    "href": "archive/linalg_mds_25/index.html",
    "title": "Linear Algebra 2025",
    "section": "",
    "text": "HSE FCS, Master of Data Science\nArchived version.\nCourse for students of the online ‚ÄúMaster of Data Science‚Äù program, implemented at the Faculty of Computer Science, HSE University.\nThe course is adaptive and aims both to introduce students to the fundamental topics of linear algebra: vector spaces, bases, linear mappings - and to demonstrate the emergence of these concepts in the practical application of linear algebra in data science and machine learning."
  },
  {
    "objectID": "archive/linalg_mds_25/index.html#course-materials",
    "href": "archive/linalg_mds_25/index.html#course-materials",
    "title": "Linear Algebra 2025",
    "section": "Course materials",
    "text": "Course materials\n\n\n    –ó–∞–Ω—è—Ç–∏–µ 1\n    \n        üìÑ –ü—Ä–µ–∑–µ–Ω—Ç–∞—Ü–∏—è ‚Ä¢ üìÑ Presentation ‚Ä¢ üìù –ó–∞–º–µ—Ç–∫–∏\n    \n    \n        Matrices and vectors. Basic operations.\n    \n\n    –ó–∞–Ω—è—Ç–∏–µ 2\n    \n        üìÑ –ü—Ä–µ–∑–µ–Ω—Ç–∞—Ü–∏—è\n    \n    \n        Vectors and vector spaces. Linear combination and span. Dependence and independence of vectors.\n    \n\n    –ó–∞–Ω—è—Ç–∏–µ 3\n    \n        üìÑ –ü—Ä–µ–∑–µ–Ω—Ç–∞—Ü–∏—è ‚Ä¢ üìù –ó–∞–º–µ—Ç–∫–∏ 1 –ø–æ—Ç–æ–∫ ‚Ä¢ üìù –ó–∞–º–µ—Ç–∫–∏ 2 –ø–æ—Ç–æ–∫\n    \n    \n        Basis of a vector space. Examples.\n    \n\n    –ó–∞–Ω—è—Ç–∏–µ 4\n    \n        üìÑ –ü—Ä–µ–∑–µ–Ω—Ç–∞—Ü–∏—è ‚Ä¢ üìù –ó–∞–º–µ—Ç–∫–∏ 1 –ø–æ—Ç–æ–∫ ‚Ä¢ üìù –ó–∞–º–µ—Ç–∫–∏ 2 –ø–æ—Ç–æ–∫ ‚Ä¢ üó∫Ô∏è Mind Map\n    \n    \n        Linear transformations. Matrix representation.\n    \n\n    –ó–∞–Ω—è—Ç–∏–µ 5\n    \n        üìÑ –ü—Ä–µ–∑–µ–Ω—Ç–∞—Ü–∏—è ‚Ä¢ üìù –ó–∞–º–µ—Ç–∫–∏ 1 –ø–æ—Ç–æ–∫ ‚Ä¢ üìù –ó–∞–º–µ—Ç–∫–∏ 2 –ø–æ—Ç–æ–∫\n    \n    \n        Change of basis as a linear transformation.\n    \n\n    –ó–∞–Ω—è—Ç–∏–µ 6\n    \n        üìù –ó–∞–º–µ—Ç–∫–∏ 1 –ø–æ—Ç–æ–∫ ‚Ä¢ üìù –ó–∞–º–µ—Ç–∫–∏ 2 –ø–æ—Ç–æ–∫\n    \n    \n        Concept of a matrix inverse. Application for a change of basis.\n    \n\n    –ó–∞–Ω—è—Ç–∏–µ 7\n    \n        üìÑ –ü—Ä–µ–∑–µ–Ω—Ç–∞—Ü–∏—è ‚Ä¢ üìù –ó–∞–º–µ—Ç–∫–∏ 1 –ø–æ—Ç–æ–∫ ‚Ä¢ üìù –ó–∞–º–µ—Ç–∫–∏ 2 –ø–æ—Ç–æ–∫\n    \n    \n        Change of a linear transformation matrix when changing bases.\n    \n\n    –ó–∞–Ω—è—Ç–∏–µ 8\n    \n        üìÑ –ü—Ä–µ–∑–µ–Ω—Ç–∞—Ü–∏—è ‚Ä¢ üìù –ó–∞–º–µ—Ç–∫–∏ 1 –ø–æ—Ç–æ–∫ ‚Ä¢ üìù –ó–∞–º–µ—Ç–∫–∏ 2 –ø–æ—Ç–æ–∫\n    \n    \n        System of linear equations. Gaussian elimination. Reduced row echelon form.\n    \n\n    –ó–∞–Ω—è—Ç–∏–µ 9\n    \n        üìÑ –ü—Ä–µ–∑–µ–Ω—Ç–∞—Ü–∏—è ‚Ä¢ üìù –ó–∞–º–µ—Ç–∫–∏ 1 –ø–æ—Ç–æ–∫ ‚Ä¢ üìù –ó–∞–º–µ—Ç–∫–∏ 2 –ø–æ—Ç–æ–∫\n    \n    \n        Inner product spaces. Normed spaces. Vector norms.\n    \n\n    –ó–∞–Ω—è—Ç–∏–µ 10\n    \n        üìÑ –ü—Ä–µ–∑–µ–Ω—Ç–∞—Ü–∏—è ‚Ä¢ üìù –ó–∞–º–µ—Ç–∫–∏ 1 –ø–æ—Ç–æ–∫ ‚Ä¢ üìù –ó–∞–º–µ—Ç–∫–∏ 2 –ø–æ—Ç–æ–∫\n    \n    \n        Back to SLEs. Usage  of  reduced row echelon form to analyze collections of vectors.\n    \n\n    –ó–∞–Ω—è—Ç–∏–µ 11\n    \n        üìù –ó–∞–º–µ—Ç–∫–∏ 1 –ø–æ—Ç–æ–∫ ‚Ä¢ üìù –ó–∞–º–µ—Ç–∫–∏ 2 –ø–æ—Ç–æ–∫ ‚Ä¢ üìù –°–ø–∏—Å–æ–∫ –∑–∞–¥–∞–Ω–∏–π\n    \n    \n        Practical session. Problems on linear transformations.\n    \n\n    –ó–∞–Ω—è—Ç–∏–µ 12\n    \n        üìù –ó–∞–º–µ—Ç–∫–∏ 1 –ø–æ—Ç–æ–∫ ‚Ä¢ üìù –ó–∞–º–µ—Ç–∫–∏ 2 –ø–æ—Ç–æ–∫\n    \n    \n        Orthogonal basis. Orthogonalization of a basis.\n    \n\n    –ó–∞–Ω—è—Ç–∏–µ \n    \n        üìù –ó–∞–º–µ—Ç–∫–∏ ‚Ä¢ üìù –°–ø–∏—Å–æ–∫ –∑–∞–¥–∞–Ω–∏–π\n    \n    \n        Additional class. Problems on change of coordinates.\n    \n\nNo matching items"
  },
  {
    "objectID": "archive/hse_mdi_prob_stat_23/probability_theory/prob_theory_main.html",
    "href": "archive/hse_mdi_prob_stat_23/probability_theory/prob_theory_main.html",
    "title": "Probability Theory 2023",
    "section": "",
    "text": "Probability Theory course program"
  },
  {
    "objectID": "archive/hse_mdi_prob_stat_23/probability_theory/prob_theory_main.html#probability-theory-hse-mdi-april-june-2023",
    "href": "archive/hse_mdi_prob_stat_23/probability_theory/prob_theory_main.html#probability-theory-hse-mdi-april-june-2023",
    "title": "Probability Theory 2023",
    "section": "",
    "text": "Probability Theory course program"
  },
  {
    "objectID": "archive/hse_mdi_prob_stat_23/math_stat/math_stat_program.html",
    "href": "archive/hse_mdi_prob_stat_23/math_stat/math_stat_program.html",
    "title": "Course program",
    "section": "",
    "text": "Probability revision: distributions. Notes."
  },
  {
    "objectID": "archive/hse_mdi_prob_stat_23/math_stat/math_stat_program.html#seminar-1.",
    "href": "archive/hse_mdi_prob_stat_23/math_stat/math_stat_program.html#seminar-1.",
    "title": "Course program",
    "section": "",
    "text": "Probability revision: distributions. Notes."
  },
  {
    "objectID": "archive.html",
    "href": "archive.html",
    "title": "Archive",
    "section": "",
    "text": "This page contains materials from completed courses."
  },
  {
    "objectID": "archive.html#section",
    "href": "archive.html#section",
    "title": "Archive",
    "section": "2025",
    "text": "2025\n\nHSE BI Courses\n\nProbability and Mathematical Statistics 2025\n\n\n\nHSE FCS Courses\n\nLinear Algebra 2025"
  },
  {
    "objectID": "archive.html#hse-mdi-courses",
    "href": "archive.html#hse-mdi-courses",
    "title": "Archive",
    "section": "2022 HSE MDI Courses",
    "text": "2022 HSE MDI Courses\n\nMathematical Statistics 2022\nProbability Theory 2022"
  },
  {
    "objectID": "archive/hse_mdi_prob_stat_23/math_stat/main.html",
    "href": "archive/hse_mdi_prob_stat_23/math_stat/main.html",
    "title": "Mathematical Statistics 2023",
    "section": "",
    "text": "Course program"
  },
  {
    "objectID": "archive/hse_mdi_prob_stat_23/math_stat/main.html#mathematical-statistics-hse-mdi-september-october-2023",
    "href": "archive/hse_mdi_prob_stat_23/math_stat/main.html#mathematical-statistics-hse-mdi-september-october-2023",
    "title": "Mathematical Statistics 2023",
    "section": "",
    "text": "Course program"
  },
  {
    "objectID": "archive/hse_mdi_prob_stat_23/probability_theory/prob_theory_program.html",
    "href": "archive/hse_mdi_prob_stat_23/probability_theory/prob_theory_program.html",
    "title": "Probability Theory course program",
    "section": "",
    "text": "Introduction. Probability space.\nProblem list\n\n\n\nProbability function axioms. Classical probability. Counting principles: multiplication rule, permutations.\nProblem list\nTheory reviews: Probability function, Counting principles.\n\n\n\nCounting principles: combinations. Conditional probability.\nProblem list.\n\n\n\nConditional probability. Independence of events. Total probability and Bayes‚Äô theorem.\nProblem list."
  },
  {
    "objectID": "archive/hse_mdi_prob_stat_23/probability_theory/prob_theory_program.html#seminar-1.",
    "href": "archive/hse_mdi_prob_stat_23/probability_theory/prob_theory_program.html#seminar-1.",
    "title": "Probability Theory course program",
    "section": "",
    "text": "Introduction. Probability space.\nProblem list"
  },
  {
    "objectID": "archive/hse_mdi_prob_stat_23/probability_theory/prob_theory_program.html#seminar-2.",
    "href": "archive/hse_mdi_prob_stat_23/probability_theory/prob_theory_program.html#seminar-2.",
    "title": "Probability Theory course program",
    "section": "",
    "text": "Probability function axioms. Classical probability. Counting principles: multiplication rule, permutations.\nProblem list\nTheory reviews: Probability function, Counting principles."
  },
  {
    "objectID": "archive/hse_mdi_prob_stat_23/probability_theory/prob_theory_program.html#seminar-3.",
    "href": "archive/hse_mdi_prob_stat_23/probability_theory/prob_theory_program.html#seminar-3.",
    "title": "Probability Theory course program",
    "section": "",
    "text": "Counting principles: combinations. Conditional probability.\nProblem list."
  },
  {
    "objectID": "archive/hse_mdi_prob_stat_23/probability_theory/prob_theory_program.html#seminar-4.",
    "href": "archive/hse_mdi_prob_stat_23/probability_theory/prob_theory_program.html#seminar-4.",
    "title": "Probability Theory course program",
    "section": "",
    "text": "Conditional probability. Independence of events. Total probability and Bayes‚Äô theorem.\nProblem list."
  },
  {
    "objectID": "archive/hse_prob_stat_22/prob_22.html",
    "href": "archive/hse_prob_stat_22/prob_22.html",
    "title": "Probability Theory 2022",
    "section": "",
    "text": "Probability Theory, HSE MDI, September ‚Äî October 2023\n\nProbability Theory course program"
  },
  {
    "objectID": "archive/hse_prob_stat_22/homework_22.html",
    "href": "archive/hse_prob_stat_22/homework_22.html",
    "title": "Home assignments",
    "section": "",
    "text": "Home assignments\n\nHome Assignment 1: Confidence Intervals, Point Estimation.\nHome Assignment 2: Hypotheses Testing."
  },
  {
    "objectID": "archive/ptms_bi_hse_25/index.html",
    "href": "archive/ptms_bi_hse_25/index.html",
    "title": "Probability and Mathematical Statistics 2025",
    "section": "",
    "text": "**HSE GSB, Business Informatics BSc*\nArchived version."
  },
  {
    "objectID": "archive/ptms_bi_hse_25/index.html#course-content",
    "href": "archive/ptms_bi_hse_25/index.html#course-content",
    "title": "Probability and Mathematical Statistics 2025",
    "section": "Course content",
    "text": "Course content\n\n\n    –ó–∞–Ω—è—Ç–∏–µ 1\n    \n        üìÑ –ü—Ä–µ–∑–µ–Ω—Ç–∞—Ü–∏—è\n    \n    \n        \n    \n    –í–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–Ω–æ–µ –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–æ. –ö–ª–∞—Å—Å–∏—á–µ—Å–∫–∞—è, –∫–æ–º–±–∏–Ω–∞—Ç–æ—Ä–Ω–∞—è –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å.\n\n    –ó–∞–Ω—è—Ç–∏–µ 2\n    \n        üìÑ –ü—Ä–µ–∑–µ–Ω—Ç–∞—Ü–∏—è\n    \n    \n        \n    \n    –£—Å–ª–æ–≤–Ω–∞—è –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å. –ü–æ–ª–Ω–∞—è –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å. –§–æ—Ä–º—É–ª–∞ –ë–∞–π–µ—Å–∞.\n\n    –ó–∞–Ω—è—Ç–∏–µ 3\n    \n        üìÑ –ü—Ä–µ–∑–µ–Ω—Ç–∞—Ü–∏—è\n    \n    \n        \n    \n    –ü–æ–ª–Ω–∞—è –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å. –§–æ—Ä–º—É–ª–∞ –ë–∞–π–µ—Å–∞. –°–ª—É—á–∞–π–Ω—ã–µ –≤–µ–ª–∏—á–∏–Ω—ã.\n\n    –ó–∞–Ω—è—Ç–∏–µ 4\n    \n        üìÑ –ü—Ä–µ–∑–µ–Ω—Ç–∞—Ü–∏—è\n    \n    \n        \n    \n    –î–∏—Å–∫—Ä–µ—Ç–Ω—ã–µ —Å–ª—É—á–∞–π–Ω—ã–µ –≤–µ–ª–∏—á–∏–Ω—ã –∏ –∏—Ö —Å–≤–æ–π—Å—Ç–≤–∞.\n\n    –ó–∞–Ω—è—Ç–∏–µ 5\n    \n        üìÑ –ü—Ä–µ–∑–µ–Ω—Ç–∞—Ü–∏—è\n    \n    \n        \n    \n    –î–∏—Å–ø–µ—Ä—Å–∏—è. –§—É–Ω–∫—Ü–∏—è —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è. –†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è –ë–µ—Ä–Ω—É–ª–ª–∏ –∏ –±–∏–Ω–æ–º–∏–∞–ª—å–Ω–æ–µ.\n\n    –ó–∞–Ω—è—Ç–∏–µ 6\n    \n        üìÑ –ü—Ä–µ–∑–µ–Ω—Ç–∞—Ü–∏—è\n    \n    \n        \n    \n    –ù–µ–ø—Ä–µ—Ä—ã–≤–Ω—ã–µ —Å–ª—É—á–∞–π–Ω—ã–µ –≤–µ–ª–∏—á–∏–Ω—ã. –†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –ü—É–∞—Å—Å–æ–Ω–∞.\n\n    –ó–∞–Ω—è—Ç–∏–µ 7\n    \n        üìÑ –ü—Ä–µ–∑–µ–Ω—Ç–∞—Ü–∏—è\n    \n    \n        \n    \n    –•–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫–∏ –Ω–µ–ø—Ä–µ—Ä—ã–≤–Ω—ã—Ö —Å–ª—É—á–∞–π–Ω—ã—Ö –≤–µ–ª–∏—á–∏–Ω. –†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è.\n\n    –ó–∞–Ω—è—Ç–∏–µ 8\n    \n        üìÑ –ü—Ä–µ–∑–µ–Ω—Ç–∞—Ü–∏—è\n    \n    \n        \n    \n    –ú–Ω–æ–≥–æ–º–µ—Ä–Ω—ã–µ —Å–ª—É—á–∞–π–Ω—ã–µ –≤–µ–ª–∏—á–∏–Ω—ã.\n\n    –ó–∞–Ω—è—Ç–∏–µ 9\n    \n        üìÑ –ü—Ä–µ–∑–µ–Ω—Ç–∞—Ü–∏—è\n    \n    \n        \n    \n    –¶–µ–Ω—Ç—Ä–∞–ª—å–Ω–∞—è –ø—Ä–µ–¥–µ–ª—å–Ω–∞—è —Ç–µ–æ—Ä–µ–º–∞.\n\n    –ó–∞–Ω—è—Ç–∏–µ 10\n    \n        üìÑ –ü—Ä–µ–∑–µ–Ω—Ç–∞—Ü–∏—è\n    \n    \n        \n    \n    –í–≤–µ–¥–µ–Ω–∏–µ –≤ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É. –í—ã–±–æ—Ä–æ—á–Ω—ã–µ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è.\n\n    –ó–∞–Ω—è—Ç–∏–µ 11\n    \n        üìÑ –ü—Ä–µ–∑–µ–Ω—Ç–∞—Ü–∏—è\n    \n    \n        \n    \n    –£—Å—Ç–æ–π—á–∏–≤–æ—Å—Ç—å –Ω–æ—Ä–º–∞–ª—å–Ω–æ–≥–æ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è. –ò–¢–ú–õ. –•–∏-–∫–≤–∞–¥—Ä–∞—Ç —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ.\n\n    –ó–∞–Ω—è—Ç–∏–µ 12\n    \n        üìÑ –ü—Ä–µ–∑–µ–Ω—Ç–∞—Ü–∏—è\n    \n    \n        \n    \n    –¢–æ—á–µ—á–Ω—ã–µ –æ—Ü–µ–Ω–∫–∏.\n\n    –ó–∞–Ω—è—Ç–∏–µ 13\n    \n        üìÑ –ü—Ä–µ–∑–µ–Ω—Ç–∞—Ü–∏—è\n    \n    \n        \n    \n    –¢–æ—á–µ—á–Ω—ã–µ –æ—Ü–µ–Ω–∫–∏. –ú–µ—Ç–æ–¥ –º–æ–º–µ–Ω—Ç–æ–≤. –ò–Ω—Ç–µ—Ä–≤–∞–ª—å–Ω—ã–µ –æ—Ü–µ–Ω–∫–∏ I.\n\n    –ó–∞–Ω—è—Ç–∏–µ 14\n    \n        üìÑ –ü—Ä–µ–∑–µ–Ω—Ç–∞—Ü–∏—è\n    \n    \n        \n    \n    –ò–Ω—Ç–µ—Ä–≤–∞–ª—å–Ω—ã–µ –æ—Ü–µ–Ω–∫–∏ II. –ò–Ω—Ç–µ—Ä–≤–∞–ª—ã –¥–ª—è –æ–¥–Ω–æ–≥–æ –ø–∞—Ä–∞–º–µ—Ç—Ä–∞. –†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –°—Ç—å—é–¥–µ–Ω—Ç–∞.\n\n    –ó–∞–Ω—è—Ç–∏–µ 15\n    \n        üìÑ –ü—Ä–µ–∑–µ–Ω—Ç–∞—Ü–∏—è\n    \n    \n        \n    \n    –ò–Ω—Ç–µ—Ä–≤–∞–ª—å–Ω—ã–µ –æ—Ü–µ–Ω–∫–∏ II. –ò–Ω—Ç–µ—Ä–≤–∞–ª—ã –¥–ª—è —Ä–∞–∑–Ω–æ—Å—Ç–∏ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤.\n\n    –ó–∞–Ω—è—Ç–∏–µ 16\n    \n        üìÑ –ü—Ä–µ–∑–µ–Ω—Ç–∞—Ü–∏—è\n    \n    \n        \n    \n    –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ —Å—Ç–∞—Ç–∏—Å—Ç–∏—á–µ—Å–∫–∏—Ö –≥–∏–ø–æ—Ç–µ–∑ I.\n\n    –ó–∞–Ω—è—Ç–∏–µ 17\n    \n        üìÑ –ü—Ä–µ–∑–µ–Ω—Ç–∞—Ü–∏—è\n    \n    \n        \n    \n    –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ —Å—Ç–∞—Ç–∏—Å—Ç–∏—á–µ—Å–∫–∏—Ö –≥–∏–ø–æ—Ç–µ–∑ II.\n\n    –ó–∞–Ω—è—Ç–∏–µ 18\n    \n        üìÑ –ü—Ä–µ–∑–µ–Ω—Ç–∞—Ü–∏—è\n    \n    \n        \n    \n    –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ —Å—Ç–∞—Ç–∏—Å—Ç–∏—á–µ—Å–∫–∏—Ö –≥–∏–ø–æ—Ç–µ–∑ III. –ù–∞—á–∞–ª–∞ –ª–∏–Ω–µ–π–Ω–æ–π —Ä–µ–≥—Ä–µ—Å—Å–∏–∏.\n\nNo matching items"
  },
  {
    "objectID": "archive/ptms_bi_hse_25/notebooks/sampling_distribution.html",
    "href": "archive/ptms_bi_hse_25/notebooks/sampling_distribution.html",
    "title": "Sampling distribution",
    "section": "",
    "text": "Code\nimport numpy as np\nimport seaborn as sns\nimport scipy.stats as stats\nfrom matplotlib import pyplot as plt\n\n\n\nThe first example: sample of dice rolls\nWhat do you see on the following picture?\nThis is a visual representation of the fact that each function from the sample, e.g.¬†sample mean, also has a random nature! And each time you generate new sample, the value of the chosen function will be different.\nTry to change sample_size variable and see what happens.\n\n\nCode\nn_draws = 100\nsample_size = 100000\nr = stats.randint(1,7)\nmu = r.mean()\nvar = r.var()\nsample_mean_realizations = np.empty(n_draws)\nrv_realizations = np.empty(n_draws)\n\n\nfor i in range(n_draws):\n    sample_realization = r.rvs(size=sample_size)\n    sample_mean_realizations[i] = np.mean(sample_realization)\n    rv_realizations[i] = r.rvs()\n\n\n\n\nCode\nfig, (ax1, ax2) = plt.subplots(nrows=2, ncols=1, figsize=(12, 8))\nax1.scatter(np.arange(n_draws), rv_realizations, label=r'single RV realization, $x$')\nax1.axhline(r.mean(), color='green', label = 'True mean')\nax2.scatter(np.arange(n_draws), sample_mean_realizations, label=r'Sample mean realization, $\\bar{x}$')\nax2.axhline(r.mean(), color='green', label = 'True mean')\n\n\nylim = ax1.get_ylim()\nax2.set_ylim(ylim)\n\nax1.legend()\nax2.legend()\nax1.set_title(r'Different random realizations of a single random variable $X$')\nax2.set_title(r'Different random realizations of sample mean $\\bar{X}$, sample size = %d' % sample_size)\nplt.show()\n\n\n\n\n\n\n\n\n\nIf on the previous picture we present just a few different realiations, in the following we generate much more samples and try to approximate the probability density function by plotting a histogram.\nAs well, try to change sample_size in below and see what happens with the histogram.\n\n\nCode\nn_draws = 1000\nsample_size = 500\nn_bins = 50\nr = stats.randint(1,7)\nsample_mean_realizations = np.empty(n_draws)\nrv_realizations = np.empty(n_draws)\n\nfor i in range(n_draws):\n    sample_realization = r.rvs(size=sample_size)\n    sample_mean_realizations[i] = np.mean(sample_realization)\n    rv_realizations[i] = r.rvs()\n\n\n\n\nCode\nfig, (ax1, ax2) = plt.subplots(nrows=2, ncols=1, figsize=(12, 8))\n\nsns.histplot(rv_realizations, bins=n_bins, ax=ax1).grid()\n\nsns.histplot(sample_mean_realizations, bins=n_bins, ax=ax2).grid()\ncounts, _, _ = plt.hist(sample_mean_realizations, bins=n_bins, alpha=0.0)  # just in order to find out the scaling coefficient for PDF\nplt.title('Histogram for standard normal variable')\n#     plt.axvline(x=np.mean(z), label='Mean of Sample Means')\n\n# scaling of normal PDF is needed, because histogram has large values on y-axis, and we need to fit them\nx_space = np.linspace(mu - 3 * var,  mu + 3 * var, 1000)\nax2.plot(x_space, np.max(counts) * stats.norm.pdf(x_space, mu, np.sqrt(var / sample_size)) * np.sqrt(2 * np.pi * var / sample_size), label='Normal density')\n# ax1.legend()\n# ax2.legend()\nax1.set_title('Histogram for realizations of a single RV')\nax2.set_title(r'Histogram for realizations of sample mean $\\bar{X}$, sample size = %d' % sample_size)\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\nThe second example: sampling from normal distribution\n\n\nCode\nr = stats.norm(5000, 200)\nn_draws = 100\nsample_size = 30\nmu = r.mean()\nvar = r.var()\nstd = r.std()\nsample_mean_realizations = np.empty(n_draws)\nrv_realizations = np.empty(n_draws)\n\n\nfor i in range(n_draws):\n    sample_realization = r.rvs(size=sample_size)\n    sample_mean_realizations[i] = np.mean(sample_realization)\n    rv_realizations[i] = r.rvs()\n\n\n\n\nCode\nfig, (ax1, ax2) = plt.subplots(nrows=2, ncols=1, figsize=(12, 8))\nax1.scatter(np.arange(n_draws), rv_realizations, label=r'single RV realization, $x$')\nax1.axhline(r.mean(), color='green', label = 'True mean')\nax1.axhline(r.mean() + std, color='blue', label = r'$\\mu + \\sigma$')\nax1.axhline(r.mean() - std, color='blue', label = r'$\\mu - \\sigma$')\n\nax2.scatter(np.arange(n_draws), sample_mean_realizations, label=r'Sample mean realization, $\\bar{x}$')\nax2.axhline(r.mean(), color='green', label = 'True mean')\nax2.axhline(r.mean() + std, color='blue', label = r'$\\mu + \\sigma$')\nax2.axhline(r.mean() - std, color='blue', label = r'$\\mu - \\sigma$')\nax1.legend(loc='best')\nax2.legend(loc='best')\nax1.set_title(r'Different random realizations of a single random variable $X \\sim N$({},{})'.format(mu,var))\nax2.set_title(r'Different random realizations of sample mean $\\bar{X}$, sample size = %d' % sample_size)\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\nCode\nn_draws = 10000\nsample_size = 1000\nn_bins = 50\nsample_mean_realizations = np.empty(n_draws)\nrv_realizations = np.empty(n_draws)\n\nfor i in range(n_draws):\n    sample_realization = r.rvs(size=sample_size)\n    sample_mean_realizations[i] = np.mean(sample_realization)\n    rv_realizations[i] = r.rvs()\n\n\n\n\nCode\nfig, (ax1, ax2) = plt.subplots(nrows=2, ncols=1, figsize=(12, 8))\n\nsns.histplot(rv_realizations, bins=n_bins, ax=ax1).grid()\n\nsns.histplot(sample_mean_realizations, bins=n_bins, ax=ax2).grid()\ncounts, _, _ = plt.hist(sample_mean_realizations, bins=n_bins, alpha=0.0)  # just in order to find out the scaling coefficient for PDF\nplt.title('Histogram for standard normal variable')\n#     plt.axvline(x=np.mean(z), label='Mean of Sample Means')\n\n# scaling of normal PDF is needed, because histogram has large values on y-axis, and we need to fit them\nx_space = np.linspace(mu - 3 * var,  mu + 3 * var, 1000)\nax2.plot(x_space, np.max(counts) * stats.norm.pdf(x_space, mu, np.sqrt(var / sample_size)) * np.sqrt(2 * np.pi * var / sample_size), label='Normal density')\nax1.legend()\nax2.legend()\nax1.set_title('Histogram for realizations of a single RV')\nax2.set_title(r'Histogram for realizations of sample mean $\\bar{X}$, sample size = %d' % sample_size)\nplt.show()\n\n\n/tmp/ipython-input-3742480432.py:13: UserWarning: No artists with labels found to put in legend.  Note that artists whose label start with an underscore are ignored when legend() is called with no argument.\n  ax1.legend()\n\n\n\n\n\n\n\n\n\n\n\nThe third example: sample variance from dice rolls\nWhat do you see on the following picture?\nThis is a visual representation of the fact that sample variance also has a random nature! Each time you generate a new sample, the value of the sample variance will be different.\nTry to change sample_size variable and see what happens.\n\n\nCode\nn_draws = 100\nsample_size = 100000\nr = stats.randint(1,7)\nmu = r.mean()\nvar = r.var()\nsample_variance_realizations = np.empty(n_draws)\nrv_realizations = np.empty(n_draws)\n\n\nfor i in range(n_draws):\n    sample_realization = r.rvs(size=sample_size)\n    sample_variance_realizations[i] = np.var(sample_realization, ddof=1)  # unbiased estimator\n    rv_realizations[i] = r.rvs()\n\n\n\n\nCode\nfig, (ax1, ax2) = plt.subplots(nrows=2, ncols=1, figsize=(12, 8))\nax1.scatter(np.arange(n_draws), rv_realizations, label=r'single RV realization, $x$')\nax1.axhline(r.mean(), color='green', label = 'True mean')\nax2.scatter(np.arange(n_draws), sample_variance_realizations, label=r'Sample variance realization, $s^2$')\nax2.axhline(r.var(), color='green', label = 'True variance')\n\n\nylim = ax1.get_ylim()\nax2.set_ylim(ylim)\n\nax1.legend()\nax2.legend()\nax1.set_title(r'Different random realizations of a single random variable $X$')\nax2.set_title(r'Different random realizations of sample variance $S^2$, sample size = %d' % sample_size)\nplt.show()\n\n\nIf on the previous picture we present just a few different realizations, in the following we generate much more samples and try to approximate the probability density function by plotting a histogram.\nAs well, try to change sample_size in below and see what happens with the histogram.\n\n\nCode\nn_draws = 1000\nsample_size = 500\nn_bins = 50\nr = stats.randint(1,7)\nsample_variance_realizations = np.empty(n_draws)\nrv_realizations = np.empty(n_draws)\n\nfor i in range(n_draws):\n    sample_realization = r.rvs(size=sample_size)\n    sample_variance_realizations[i] = np.var(sample_realization, ddof=1)  # unbiased estimator\n    rv_realizations[i] = r.rvs()\n\n\n\n\nCode\nfig, (ax1, ax2) = plt.subplots(nrows=2, ncols=1, figsize=(12, 8))\n\nsns.histplot(rv_realizations, bins=n_bins, ax=ax1).grid()\n\nsns.histplot(sample_variance_realizations, bins=n_bins, ax=ax2).grid()\ncounts, _, _ = plt.hist(sample_variance_realizations, bins=n_bins, alpha=0.0)  # just in order to find out the scaling coefficient for PDF\n\n# For large sample sizes, sample variance is approximately normal\n# E[S^2] = œÉ¬≤, Var[S^2] ‚âà 2œÉ‚Å¥/(n-1) for normal distribution\n# For uniform distribution, the approximation is less accurate but still useful\nx_space = np.linspace(0, 3 * var, 1000)\nax2.plot(x_space, np.max(counts) * stats.norm.pdf(x_space, var, np.sqrt(2 * var**2 / (sample_size - 1))) * np.sqrt(2 * np.pi * 2 * var**2 / (sample_size - 1)), label='Normal approximation', color='red')\n\nax1.set_title('Histogram for realizations of a single RV')\nax2.set_title(r'Histogram for realizations of sample variance $S^2$, sample size = %d' % sample_size)\nax2.axvline(var, color='green', linestyle='--', label='True variance')\nax2.legend()\nplt.show()\n\n\n\n\nThe fourth example: sample variance from normal distribution\n\n\nCode\nr = stats.norm(5000, 200)\nn_draws = 100\nsample_size = 30\nmu = r.mean()\nvar = r.var()\nstd = r.std()\nsample_variance_realizations = np.empty(n_draws)\nrv_realizations = np.empty(n_draws)\n\n\nfor i in range(n_draws):\n    sample_realization = r.rvs(size=sample_size)\n    sample_variance_realizations[i] = np.var(sample_realization, ddof=1)  # unbiased estimator\n    rv_realizations[i] = r.rvs()\n\n\n\n\nCode\nfig, (ax1, ax2) = plt.subplots(nrows=2, ncols=1, figsize=(12, 8))\nax1.scatter(np.arange(n_draws), rv_realizations, label=r'single RV realization, $x$')\nax1.axhline(r.mean(), color='green', label = 'True mean')\nax1.axhline(r.mean() + std, color='blue', label = r'$\\mu + \\sigma$')\nax1.axhline(r.mean() - std, color='blue', label = r'$\\mu - \\sigma$')\n\nax2.scatter(np.arange(n_draws), sample_variance_realizations, label=r'Sample variance realization, $s^2$')\nax2.axhline(r.var(), color='green', label = 'True variance')\nax2.axhline(r.var() + np.sqrt(2 * var**2 / (sample_size - 1)), color='blue', label = r'$\\sigma^2 + \\sqrt{Var(S^2)}$')\nax2.axhline(r.var() - np.sqrt(2 * var**2 / (sample_size - 1)), color='blue', label = r'$\\sigma^2 - \\sqrt{Var(S^2)}$')\nax1.legend(loc='best')\nax2.legend(loc='best')\nax1.set_title(r'Different random realizations of a single random variable $X \\sim N$({},{})'.format(mu,var))\nax2.set_title(r'Different random realizations of sample variance $S^2$, sample size = %d' % sample_size)\nplt.show()\n\n\n\n\nCode\nn_draws = 10000\nsample_size = 1000\nn_bins = 50\nsample_variance_realizations = np.empty(n_draws)\nrv_realizations = np.empty(n_draws)\n\nfor i in range(n_draws):\n    sample_realization = r.rvs(size=sample_size)\n    sample_variance_realizations[i] = np.var(sample_realization, ddof=1)  # unbiased estimator\n    rv_realizations[i] = r.rvs()\n\n\n\n\nCode\nfig, (ax1, ax2) = plt.subplots(nrows=2, ncols=1, figsize=(12, 8))\n\nsns.histplot(rv_realizations, bins=n_bins, ax=ax1).grid()\n\nsns.histplot(sample_variance_realizations, bins=n_bins, ax=ax2).grid()\ncounts, _, _ = plt.hist(sample_variance_realizations, bins=n_bins, alpha=0.0)  # just in order to find out the scaling coefficient for PDF\n\n# For normal distribution, (n-1)*S¬≤/œÉ¬≤ ~ œá¬≤(n-1)\n# So S¬≤ ~ (œÉ¬≤/(n-1)) * œá¬≤(n-1)\n# E[S¬≤] = œÉ¬≤, Var[S¬≤] = 2œÉ‚Å¥/(n-1)\nx_space = np.linspace(var - 3 * np.sqrt(2 * var**2 / (sample_size - 1)), var + 3 * np.sqrt(2 * var**2 / (sample_size - 1)), 1000)\nax2.plot(x_space, np.max(counts) * stats.norm.pdf(x_space, var, np.sqrt(2 * var**2 / (sample_size - 1))) * np.sqrt(2 * np.pi * 2 * var**2 / (sample_size - 1)), label='Normal approximation', color='red')\n\nax1.set_title('Histogram for realizations of a single RV')\nax2.set_title(r'Histogram for realizations of sample variance $S^2$, sample size = %d' % sample_size)\nax2.axvline(var, color='green', linestyle='--', label='True variance')\nax2.legend()\nplt.show()"
  },
  {
    "objectID": "archive/hse_prob_stat_21/program.html",
    "href": "archive/hse_prob_stat_21/program.html",
    "title": "üöÄ Statistics course program",
    "section": "",
    "text": "Introduction. A brief review of the main ideas of the previous course on probability theory. Random variables.\n\n\n\nRandom variables and distributions.\n\n\n\nJoint distributions 1.\n\n\n\nMaterials. Notes. Video.\nJoint distributions 2\n\n\n\nInferential statistics. Introduction, motivation, main problem types. Sampling distributions. Point estimators, their efficiency. Mean squared error.\nMaterials. Annotated materials. Notes. Video.\n\n\n\nConfidence intervals. Introduction. CLT. CI for proportions.\nMaterials. Annotated materials. Notes. Video.\n\n\n\nConfidence intervals. Chi-squared distribution. Student‚Äôs t-distribution.\nNotes. Video.\n\n\n\nHypothesis testing: introduction, motivation, definitions and problem statements. Materials are below for the Seminar 10.\n\n\n\nHypothesis testing: two sample, known and unknown population variances.\nMaterials. Notes. Video.\n\n\n\nWe covered all problems in demo variant up to Hypothesis Testing in the free response section.\nNotes. Video."
  },
  {
    "objectID": "archive/hse_prob_stat_21/program.html#seminar-1.",
    "href": "archive/hse_prob_stat_21/program.html#seminar-1.",
    "title": "üöÄ Statistics course program",
    "section": "",
    "text": "Introduction. A brief review of the main ideas of the previous course on probability theory. Random variables."
  },
  {
    "objectID": "archive/hse_prob_stat_21/program.html#seminar-2.",
    "href": "archive/hse_prob_stat_21/program.html#seminar-2.",
    "title": "üöÄ Statistics course program",
    "section": "",
    "text": "Random variables and distributions."
  },
  {
    "objectID": "archive/hse_prob_stat_21/program.html#seminar-3.",
    "href": "archive/hse_prob_stat_21/program.html#seminar-3.",
    "title": "üöÄ Statistics course program",
    "section": "",
    "text": "Joint distributions 1."
  },
  {
    "objectID": "archive/hse_prob_stat_21/program.html#seminar-4.",
    "href": "archive/hse_prob_stat_21/program.html#seminar-4.",
    "title": "üöÄ Statistics course program",
    "section": "",
    "text": "Materials. Notes. Video.\nJoint distributions 2"
  },
  {
    "objectID": "archive/hse_prob_stat_21/program.html#seminar-6.",
    "href": "archive/hse_prob_stat_21/program.html#seminar-6.",
    "title": "üöÄ Statistics course program",
    "section": "",
    "text": "Inferential statistics. Introduction, motivation, main problem types. Sampling distributions. Point estimators, their efficiency. Mean squared error.\nMaterials. Annotated materials. Notes. Video."
  },
  {
    "objectID": "archive/hse_prob_stat_21/program.html#seminar-7.",
    "href": "archive/hse_prob_stat_21/program.html#seminar-7.",
    "title": "üöÄ Statistics course program",
    "section": "",
    "text": "Confidence intervals. Introduction. CLT. CI for proportions.\nMaterials. Annotated materials. Notes. Video."
  },
  {
    "objectID": "archive/hse_prob_stat_21/program.html#seminar-8.",
    "href": "archive/hse_prob_stat_21/program.html#seminar-8.",
    "title": "üöÄ Statistics course program",
    "section": "",
    "text": "Confidence intervals. Chi-squared distribution. Student‚Äôs t-distribution.\nNotes. Video."
  },
  {
    "objectID": "archive/hse_prob_stat_21/program.html#seminar-9.",
    "href": "archive/hse_prob_stat_21/program.html#seminar-9.",
    "title": "üöÄ Statistics course program",
    "section": "",
    "text": "Hypothesis testing: introduction, motivation, definitions and problem statements. Materials are below for the Seminar 10."
  },
  {
    "objectID": "archive/hse_prob_stat_21/program.html#seminar-10.",
    "href": "archive/hse_prob_stat_21/program.html#seminar-10.",
    "title": "üöÄ Statistics course program",
    "section": "",
    "text": "Hypothesis testing: two sample, known and unknown population variances.\nMaterials. Notes. Video."
  },
  {
    "objectID": "archive/hse_prob_stat_21/program.html#exam-consultation-1.",
    "href": "archive/hse_prob_stat_21/program.html#exam-consultation-1.",
    "title": "üöÄ Statistics course program",
    "section": "",
    "text": "We covered all problems in demo variant up to Hypothesis Testing in the free response section.\nNotes. Video."
  },
  {
    "objectID": "archive/hse_prob_stat_21/homework.html",
    "href": "archive/hse_prob_stat_21/homework.html",
    "title": "üíÄ Home assignments",
    "section": "",
    "text": "Home assignment on Hypothesis Testing.\n\nProbability Theory and Statistics. MDI, Fall 2021.\n\nA sample of seven is taken at random from a large batch of (nominally 12 volt) batteries. These are tested and their true voltages are shown below:\n\n\\[\n12.9 \\quad 11.6 \\quad 13.5 \\quad 13.9 \\quad 12.1 \\quad 11.9 \\quad 13.0.\n\\]\n\nTest if the mean batch voltage is less than 12 volts.\n\n\nIf you live in California, the decision to purchase earthquake insurance is a critical one. An article in the Annals of the Association of American Geographers (June 1992) investigated many factors that California residents consider when purchasing earthquake insurance. The survey revealed that only \\(133\\) of \\(337\\) randomly selected residences in Los Angeles County were protected by earthquake insurance.\n\nWhat are the appropriate null and alternative hypotheses to test the research hypothesis that less than \\(40\\)% of the residents of Los Angeles County were protected by earthquake insurance?\nDoes the data provide sufficient evidence to support the research hypothesis? (Use \\(\\alpha = 0.10\\))\n\nThe American Hospital Association reports in Hospital Statistics that the mean cost to general community hospitals per patient per day in U.S. hospitals was $ $951$ in \\(1998\\). In that same year, a random sample of 30 daily costs in New York City hospitals yielded a mean of $ $1185$. Assuming a population standard deviation of $ $333$ for New York City hospitals, do the data provide sufficient evidence to conclude that in \\(1998\\) the mean cost in NYC hospitals exceeded the national mean of $ $951$? Perform the required hypothesis test at the \\(5\\)% significance level.\nA random sample of \\(1562\\) undergraduates enrolled in marketing courses were asked to respond on a scale from one to seven to the proposition ‚ÄòAdvertising helps to raise our standard of living.‚Äô The sample mean response was \\(4.27\\) and the sample standard deviation was \\(1.32\\). Test at the \\(1\\)% level, against a two-sided alternative, the null hypothesis that the population mean is \\(4\\).\nA random sample of ten students found the following figures, in hours, for time spent studying in the week before the final exams:\n\n\\[\n28 \\quad 57 \\quad 42 \\quad 35 \\quad 61 \\quad 39 \\quad 55 \\quad 46 \\quad 49 \\quad 38.\n\\]\nAssume that the population distribution is normal.\n\nFind the sample mean and sample standard deviation.\nTest at the \\(5\\)% significance level the null hypothesis that the population mean is \\(40\\) against the alternative that it is higher.\n\n\nThe data in the following table show the numbers of daily parking offences in two areas of a city. The day identifications are unknown and the recordings were not necessarily made on the same days. Is there evidence that the areas experience different mean numbers of offences?\n\n\n\n\nArea A\nArea B\n\n\n\n\n38\n32\n\n\n38\n38\n\n\n29\n22\n\n\n45\n30\n\n\n42\n34\n\n\n33\n28\n\n\n27\n32\n\n\n32\n34\n\n\n32\n24\n\n\n34\nno data\n\n\n\n\n(Optional but not hard though) Random variable \\(X\\) has a normal distribution \\(N(\\mu, \\sigma^2)\\). Let \\(\\sigma\\) be equal to \\(25\\), and sample size \\(n\\) be equal to \\(100\\). You test null hypothesis \\(H_0: \\mu=100\\) against the alternative hypothesis \\(H_1: \\mu = \\mu_1 &lt; 100\\). Significance level of the test should be \\(\\alpha = 10\\%\\).\n\n\nDefine a test with critical region \\(C = {(X_1 , \\ldots , X_n ) : \\bar{X} &lt; k}\\). Find the decision boundary for such a test.\nFormulate definition of the power function \\(W(\\theta)\\) of the statistical test. What does it show if \\(\\theta \\in \\Theta_0 (H_0)\\)? What does it show if \\(\\theta \\in \\Theta_1 (H_1)\\)?\nWrite down analytically a power function for the test you obtained.\nDraw the plot of this power function. You can sketch it on a paper just by few calculated points, or use Python (or any tool that can calculate functions and draw plots) for the explicit version."
  },
  {
    "objectID": "math_stat_shared/stat_links.html",
    "href": "math_stat_shared/stat_links.html",
    "title": "External materials",
    "section": "",
    "text": "Table for the standard normal distribution values: üîÆ.\nTables for the Student‚Äôs t-distribution values: ‚û°, and ‚û°.\nCalculator of the t-score: üíª.\n\n\n\n\n\nMIT brief but well-written review on joint distributions\nNice introductory coverage of confidence intervals\nNote that in the subtopic ‚ÄúComparing Two Means‚Äù there is given the most complicated case, when we do not know variances \\(\\sigma_1^2\\) and \\(\\sigma_2^2\\) of two populations, moreover they may not be equal to each other. We didn‚Äôt cover this case, and it‚Äôs better here to follow our materials.\nNice introductory coverage of hypothesis testing, which can help you to look at the HT from slightly another position of the different author. Please, also note here, that there is not full coverage of the subtopic ‚ÄúTests on Differences of Population Means‚Äù. It is better here, as well, to follow our materials.\nYou can find interesting articles on the website statisticsbyjim.com. Among others I can recommend one graphically representing \\(\\alpha\\) and \\(p\\)-values, article about how hypothesis testing works in general, and the article devoted to significance levels."
  },
  {
    "objectID": "math_stat_shared/stat_links.html#tables-and-cdf-calculation-tools",
    "href": "math_stat_shared/stat_links.html#tables-and-cdf-calculation-tools",
    "title": "External materials",
    "section": "",
    "text": "Table for the standard normal distribution values: üîÆ.\nTables for the Student‚Äôs t-distribution values: ‚û°, and ‚û°.\nCalculator of the t-score: üíª."
  },
  {
    "objectID": "math_stat_shared/stat_links.html#recommended-external-information-sources",
    "href": "math_stat_shared/stat_links.html#recommended-external-information-sources",
    "title": "External materials",
    "section": "",
    "text": "MIT brief but well-written review on joint distributions\nNice introductory coverage of confidence intervals\nNote that in the subtopic ‚ÄúComparing Two Means‚Äù there is given the most complicated case, when we do not know variances \\(\\sigma_1^2\\) and \\(\\sigma_2^2\\) of two populations, moreover they may not be equal to each other. We didn‚Äôt cover this case, and it‚Äôs better here to follow our materials.\nNice introductory coverage of hypothesis testing, which can help you to look at the HT from slightly another position of the different author. Please, also note here, that there is not full coverage of the subtopic ‚ÄúTests on Differences of Population Means‚Äù. It is better here, as well, to follow our materials.\nYou can find interesting articles on the website statisticsbyjim.com. Among others I can recommend one graphically representing \\(\\alpha\\) and \\(p\\)-values, article about how hypothesis testing works in general, and the article devoted to significance levels."
  },
  {
    "objectID": "math_stat_shared/notebooks/t_chi2_density.html",
    "href": "math_stat_shared/notebooks/t_chi2_density.html",
    "title": "",
    "section": "",
    "text": "CodeShow All CodeHide All Code\n\n\n\n\n\n\nCode\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy.stats import chi2, t, norm\n\n\n\n\nCode\nplt.figure(figsize=(15,10))\nx = np.arange(0, 20, 0.001)\n\nfor i in range(3, 10):\n    plt.plot(x, chi2.pdf(x, df=i), label='df: {}'.format(i))\nplt.rc('legend', fontsize=15)\nplt.legend(title='Parameters')\n\nplt.ylabel('Density',fontsize=20)\nplt.xlabel('x', fontsize=20)\nplt.title('Chi-Square Distributions', fontsize=20)\nplt.savefig(\"chi_square_density.pdf\")\n\n\n\n\nCode\nplt.figure(figsize=(12,8))\nx = np.arange(-5, 5, 0.001)\n\nfor i in range(1,8):\n    plt.plot(x, t.pdf(x, df=i), label='df: {}'.format(i))\n    \nplt.rc('legend', fontsize=20)\nplt.legend(title='Parameters', title_fontsize=15)\n\nplt.ylabel('Density',fontsize=20)\nplt.xlabel('x', fontsize=20)\nplt.title('Student t-distributions', fontsize=20)\nplt.grid(True)\nplt.tight_layout()\nplt.savefig(\"t_density.pdf\")\n\n\n\n\nCode\nplt.figure(figsize=(15,10))\nd_f = 2\nplt.plot(x, t.pdf(x, df= d_f), label='df: {}'.format(d_f))\nplt.plot(x, norm.pdf(x), label='Z')    \nplt.legend(title='Parameters')\n\nplt.ylabel('Density')\nplt.xlabel('x')\nplt.title('Student t-distribution vs Standard Normal', fontsize=14)\n\n\n\n\nCode\nplt.figure(figsize=(12,8))\nd_f = 100\nplt.plot(x, t.pdf(x, df= d_f), label='t-distribution, df: {}'.format(d_f))\n\nplt.plot(x, norm.pdf(x), label='Standard Normal')  \n\nd_f = 2\nplt.plot(x, t.pdf(x, df= d_f), label='t-distribution, df: {}'.format(d_f))\n\nplt.rc('legend', fontsize=15)\nplt.legend()\n\nplt.ylabel('Density',fontsize=20)\nplt.xlabel('x', fontsize=20)\nplt.title('Student t-distribution vs Standard Normal', fontsize=20)\nplt.grid(True)\nplt.tight_layout()\nplt.savefig(\"t_limit_z.pdf\")"
  },
  {
    "objectID": "index.html#section",
    "href": "index.html#section",
    "title": "Archive",
    "section": "2025",
    "text": "2025\n\nHSE BI Courses\n\nProbability and Mathematical Statistics 2025\n\n\n\nHSE FCS Courses\n\nLinear Algebra 2025"
  },
  {
    "objectID": "index.html#hse-mdi-courses",
    "href": "index.html#hse-mdi-courses",
    "title": "Archive",
    "section": "2022 HSE MDI Courses",
    "text": "2022 HSE MDI Courses\n\nMathematical Statistics 2022\nProbability Theory 2022"
  }
]